{"id": "2602.17768", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17768", "abs": "https://arxiv.org/abs/2602.17768", "authors": ["Boda Lin", "Yongjie Zhu", "Xiaocheng Gong", "Wenyu Qin", "Meng Wang"], "title": "KPM-Bench: A Kinematic Parsing Motion Benchmark for Fine-grained Motion-centric Video Understanding", "comment": "26 pages", "summary": "Despite recent advancements, video captioning models still face significant limitations in accurately describing fine-grained motion details and suffer from severe hallucination issues. These challenges become particularly prominent when generating captions for motion-centric videos, where precise depiction of intricate movements and limb dynamics is crucial yet often neglected. To alleviate this gap, we introduce an automated annotation pipeline that integrates kinematic-based motion computation with linguistic parsing, enabling detailed decomposition and description of complex human motions. Based on this pipeline, we construct and release the Kinematic Parsing Motion Benchmark (KPM-Bench), a novel open-source dataset designed to facilitate fine-grained motion understanding. KPM-Bench consists of (i) fine-grained video-caption pairs that comprehensively illustrate limb-level dynamics in complex actions, (ii) diverse and challenging question-answer pairs focusing specifically on motion understanding, and (iii) a meticulously curated evaluation set specifically designed to assess hallucination phenomena associated with motion descriptions. Furthermore, to address hallucination issues systematically, we propose the linguistically grounded Motion Parsing and Extraction (MoPE) algorithm, capable of accurately extracting motion-specific attributes directly from textual captions. Leveraging MoPE, we introduce a precise hallucination evaluation metric that functions independently of large-scale vision-language or language-only models. By integrating MoPE into the GRPO post-training framework, we effectively mitigate hallucination problems, significantly improving the reliability of motion-centric video captioning models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u89c6\u9891\u5b57\u5e55\u751f\u6210\u4e2d\u7ec6\u7c92\u5ea6\u8fd0\u52a8\u63cf\u8ff0\u4e0d\u51c6\u786e\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u51fa\u4e86KPM-Bench\u6570\u636e\u96c6\u548cMoPE\u7b97\u6cd5\uff0c\u901a\u8fc7\u8fd0\u52a8\u89e3\u6790\u548c\u63d0\u53d6\u6280\u672f\u6539\u5584\u8fd0\u52a8\u4e2d\u5fc3\u89c6\u9891\u7684\u5b57\u5e55\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5b57\u5e55\u6a21\u578b\u5728\u63cf\u8ff0\u7ec6\u7c92\u5ea6\u8fd0\u52a8\u7ec6\u8282\u65f6\u5b58\u5728\u4e25\u91cd\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u8fd0\u52a8\u4e2d\u5fc3\u89c6\u9891\u4e2d\uff0c\u5bf9\u590d\u6742\u52a8\u4f5c\u548c\u80a2\u4f53\u52a8\u6001\u7684\u7cbe\u786e\u63cf\u8ff0\u4e0d\u8db3\uff0c\u4e14\u5b58\u5728\u4e25\u91cd\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "method": "1) \u5f00\u53d1\u81ea\u52a8\u5316\u6807\u6ce8\u6d41\u7a0b\uff0c\u7ed3\u5408\u8fd0\u52a8\u5b66\u8ba1\u7b97\u548c\u8bed\u8a00\u89e3\u6790\uff1b2) \u6784\u5efaKPM-Bench\u6570\u636e\u96c6\uff0c\u5305\u542b\u7ec6\u7c92\u5ea6\u89c6\u9891-\u5b57\u5e55\u5bf9\u3001\u8fd0\u52a8\u7406\u89e3\u95ee\u7b54\u5bf9\u548c\u5e7b\u89c9\u8bc4\u4f30\u96c6\uff1b3) \u63d0\u51faMoPE\u7b97\u6cd5\u4ece\u6587\u672c\u5b57\u5e55\u4e2d\u63d0\u53d6\u8fd0\u52a8\u7279\u5b9a\u5c5e\u6027\uff1b4) \u5c06MoPE\u96c6\u6210\u5230GRPO\u540e\u8bad\u7ec3\u6846\u67b6\u4e2d\u3002", "result": "\u521b\u5efa\u4e86KPM-Bench\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8eMoPE\u7684\u5e7b\u89c9\u8bc4\u4f30\u6307\u6807\uff0c\u901a\u8fc7GRPO\u6846\u67b6\u6709\u6548\u7f13\u89e3\u4e86\u5e7b\u89c9\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8fd0\u52a8\u4e2d\u5fc3\u89c6\u9891\u5b57\u5e55\u751f\u6210\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u96c6\u6784\u5efa\u548c\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u7cfb\u7edf\u89e3\u51b3\u4e86\u89c6\u9891\u5b57\u5e55\u751f\u6210\u4e2d\u7684\u7ec6\u7c92\u5ea6\u8fd0\u52a8\u63cf\u8ff0\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u4e3a\u8fd0\u52a8\u7406\u89e3\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u57fa\u51c6\u548c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17770", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17770", "abs": "https://arxiv.org/abs/2602.17770", "authors": ["Balamurugan Thambiraja", "Omid Taheri", "Radek Danecek", "Giorgio Becherini", "Gerard Pons-Moll", "Justus Thies"], "title": "CLUTCH: Contextualized Language model for Unlocking Text-Conditioned Hand motion modelling in the wild", "comment": "ICLR2026; Project page: https://balamuruganthambiraja.github.io/CLUTCH/", "summary": "Hands play a central role in daily life, yet modeling natural hand motions remains underexplored. Existing methods that tackle text-to-hand-motion generation or hand animation captioning rely on studio-captured datasets with limited actions and contexts, making them costly to scale to \"in-the-wild\" settings. Further, contemporary models and their training schemes struggle to capture animation fidelity with text-motion alignment. To address this, we (1) introduce '3D Hands in the Wild' (3D-HIW), a dataset of 32K 3D hand-motion sequences and aligned text, and (2) propose CLUTCH, an LLM-based hand animation system with two critical innovations: (a) SHIFT, a novel VQ-VAE architecture to tokenize hand motion, and (b) a geometric refinement stage to finetune the LLM. To build 3D-HIW, we propose a data annotation pipeline that combines vision-language models (VLMs) and state-of-the-art 3D hand trackers, and apply it to a large corpus of egocentric action videos covering a wide range of scenarios. To fully capture motion in-the-wild, CLUTCH employs SHIFT, a part-modality decomposed VQ-VAE, which improves generalization and reconstruction fidelity. Finally, to improve animation quality, we introduce a geometric refinement stage, where CLUTCH is co-supervised with a reconstruction loss applied directly to decoded hand motion parameters. Experiments demonstrate state-of-the-art performance on text-to-motion and motion-to-text tasks, establishing the first benchmark for scalable in-the-wild hand motion modelling. Code, data and models will be released.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e863D-HIW\u6570\u636e\u96c6\u548cCLUTCH\u7cfb\u7edf\uff0c\u7528\u4e8e\u89e3\u51b3\u91ce\u5916\u73af\u5883\u4e0b\u6587\u672c\u4e0e\u624b\u90e8\u52a8\u4f5c\u751f\u6210\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u5728\u6587\u672c\u5230\u624b\u90e8\u52a8\u4f5c\u751f\u6210\u548c\u52a8\u4f5c\u63cf\u8ff0\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u624b\u90e8\u52a8\u4f5c\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5de5\u4f5c\u5ba4\u91c7\u96c6\u7684\u6570\u636e\u96c6\uff0c\u52a8\u4f5c\u548c\u4e0a\u4e0b\u6587\u6709\u9650\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u91ce\u5916\u73af\u5883\u3002\u540c\u65f6\uff0c\u5f53\u524d\u6a21\u578b\u5728\u52a8\u753b\u4fdd\u771f\u5ea6\u548c\u6587\u672c-\u52a8\u4f5c\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "1) \u6784\u5efa3D-HIW\u6570\u636e\u96c6\uff1a\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5148\u8fdb3D\u624b\u90e8\u8ddf\u8e2a\u5668\uff0c\u4ece\u5927\u91cf\u7b2c\u4e00\u4eba\u79f0\u52a8\u4f5c\u89c6\u9891\u4e2d\u63d0\u53d632K\u4e2a3D\u624b\u90e8\u52a8\u4f5c\u5e8f\u5217\u548c\u5bf9\u5e94\u6587\u672c\uff1b2) \u63d0\u51faCLUTCH\u7cfb\u7edf\uff1a\u5305\u542bSHIFT\uff08\u57fa\u4e8eVQ-VAE\u7684\u624b\u90e8\u52a8\u4f5c\u5206\u8bcd\u67b6\u6784\uff09\u548c\u51e0\u4f55\u7cbe\u70bc\u9636\u6bb5\uff08\u901a\u8fc7\u91cd\u5efa\u635f\u5931\u5fae\u8c03LLM\uff09\u3002", "result": "\u5728\u6587\u672c\u5230\u624b\u90e8\u52a8\u4f5c\u751f\u6210\u548c\u52a8\u4f5c\u5230\u6587\u672c\u63cf\u8ff0\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u7684\u91ce\u5916\u624b\u90e8\u52a8\u4f5c\u5efa\u6a21\u7684\u9996\u4e2a\u57fa\u51c6\u3002", "conclusion": "\u901a\u8fc73D-HIW\u6570\u636e\u96c6\u548cCLUTCH\u7cfb\u7edf\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u91ce\u5916\u73af\u5883\u4e0b\u624b\u90e8\u52a8\u4f5c\u5efa\u6a21\u7684\u6311\u6218\uff0c\u4e3a\u6587\u672c\u4e0e\u624b\u90e8\u52a8\u4f5c\u7684\u76f8\u4e92\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17793", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.17793", "abs": "https://arxiv.org/abs/2602.17793", "authors": ["Peide Zhu", "Linbin Lu", "Zhiqin Chen", "Xiong Chen"], "title": "LGD-Net: Latent-Guided Dual-Stream Network for HER2 Scoring with Task-Specific Domain Knowledge", "comment": null, "summary": "It is a critical task to evalaute HER2 expression level accurately for breast cancer evaluation and targeted treatment therapy selection. However, the standard multi-step Immunohistochemistry (IHC) staining is resource-intensive, expensive, and time-consuming, which is also often unavailable in many areas. Consequently, predicting HER2 levels directly from H&E slides has emerged as a potential alternative solution. It has been shown to be effective to use virtual IHC images from H&E images for automatic HER2 scoring. However, the pixel-level virtual staining methods are computationally expensive and prone to reconstruction artifacts that can propagate diagnostic errors. To address these limitations, we propose the Latent-Guided Dual-Stream Network (LGD-Net), a novel framework that employes cross-modal feature hallucination instead of explicit pixel-level image generation. LGD-Net learns to map morphological H&E features directly to the molecular latent space, guided by a teacher IHC encoder during training. To ensure the hallucinated features capture clinically relevant phenotypes, we explicitly regularize the model training with task-specific domain knowledge, specifically nuclei distribution and membrane staining intensity, via lightweight auxiliary regularization tasks. Extensive experiments on the public BCI dataset demonstrate that LGD-Net achieves state-of-the-art performance, significantly outperforming baseline methods while enabling efficient inference using single-modality H&E inputs.", "AI": {"tldr": "\u63d0\u51faLGD-Net\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u7279\u5f81\u5e7b\u89c9\u800c\u975e\u50cf\u7d20\u7ea7\u56fe\u50cf\u751f\u6210\uff0c\u76f4\u63a5\u4eceH&E\u5207\u7247\u9884\u6d4bHER2\u8868\u8fbe\u6c34\u5e73\uff0c\u907f\u514d\u91cd\u5efa\u4f2a\u5f71\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387", "motivation": "\u4f20\u7edfIHC\u67d3\u8272\u65b9\u6cd5\u8d44\u6e90\u5bc6\u96c6\u3001\u6602\u8d35\u4e14\u8017\u65f6\uff0c\u8bb8\u591a\u5730\u533a\u65e0\u6cd5\u83b7\u5f97\u3002\u4eceH&E\u5207\u7247\u76f4\u63a5\u9884\u6d4bHER2\u6c34\u5e73\u662f\u6f5c\u5728\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u50cf\u7d20\u7ea7\u865a\u62df\u67d3\u8272\u65b9\u6cd5\u8ba1\u7b97\u6602\u8d35\u4e14\u6613\u4ea7\u751f\u91cd\u5efa\u4f2a\u5f71\uff0c\u53ef\u80fd\u4f20\u64ad\u8bca\u65ad\u9519\u8bef", "method": "\u63d0\u51faLatent-Guided Dual-Stream Network (LGD-Net)\uff0c\u91c7\u7528\u8de8\u6a21\u6001\u7279\u5f81\u5e7b\u89c9\u800c\u975e\u663e\u5f0f\u50cf\u7d20\u7ea7\u56fe\u50cf\u751f\u6210\u3002\u6a21\u578b\u5b66\u4e60\u5c06\u5f62\u6001\u5b66H&E\u7279\u5f81\u76f4\u63a5\u6620\u5c04\u5230\u5206\u5b50\u6f5c\u5728\u7a7a\u95f4\uff0c\u5728\u8bad\u7ec3\u65f6\u7531\u6559\u5e08IHC\u7f16\u7801\u5668\u5f15\u5bfc\u3002\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8f85\u52a9\u6b63\u5219\u5316\u4efb\u52a1\uff0c\u5229\u7528\u7279\u5b9a\u9886\u57df\u77e5\u8bc6\uff08\u6838\u5206\u5e03\u548c\u819c\u67d3\u8272\u5f3a\u5ea6\uff09\u6b63\u5219\u5316\u6a21\u578b\u8bad\u7ec3", "result": "\u5728\u516c\u5f00BCI\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cLGD-Net\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u80fd\u591f\u4f7f\u7528\u5355\u6a21\u6001H&E\u8f93\u5165\u8fdb\u884c\u9ad8\u6548\u63a8\u7406", "conclusion": "LGD-Net\u901a\u8fc7\u8de8\u6a21\u6001\u7279\u5f81\u5e7b\u89c9\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u50cf\u7d20\u7ea7\u865a\u62df\u67d3\u8272\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u4eceH&E\u5207\u7247\u51c6\u786e\u9884\u6d4bHER2\u8868\u8fbe\u6c34\u5e73\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.17799", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17799", "abs": "https://arxiv.org/abs/2602.17799", "authors": ["Jose Sosa", "Danila Rukhovich", "Anis Kacem", "Djamila Aouada"], "title": "Enabling Training-Free Text-Based Remote Sensing Segmentation", "comment": null, "summary": "Recent advances in Vision Language Models (VLMs) and Vision Foundation Models (VFMs) have opened new opportunities for zero-shot text-guided segmentation of remote sensing imagery. However, most existing approaches still rely on additional trainable components, limiting their generalisation and practical applicability. In this work, we investigate to what extent text-based remote sensing segmentation can be achieved without additional training, by relying solely on existing foundation models. We propose a simple yet effective approach that integrates contrastive and generative VLMs with the Segment Anything Model (SAM), enabling a fully training-free or lightweight LoRA-tuned pipeline. Our contrastive approach employs CLIP as mask selector for SAM's grid-based proposals, achieving state-of-the-art open-vocabulary semantic segmentation (OVSS) in a completely zero-shot setting. In parallel, our generative approach enables reasoning and referring segmentation by generating click prompts for SAM using GPT-5 in a zero-shot setting and a LoRA-tuned Qwen-VL model, with the latter yielding the best results. Extensive experiments across 19 remote sensing benchmarks, including open-vocabulary, referring, and reasoning-based tasks, demonstrate the strong capabilities of our approach. Code will be released at https://github.com/josesosajs/trainfree-rs-segmentation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u4ec5\u9700\u8f7b\u91cfLoRA\u8c03\u4f18\u7684\u9065\u611f\u56fe\u50cf\u6587\u672c\u5f15\u5bfc\u5206\u5272\u65b9\u6cd5\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5f0f\u548c\u751f\u6210\u5f0f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0eSAM\uff0c\u572819\u4e2a\u9065\u611f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e3a\u96f6\u6837\u672c\u6587\u672c\u5f15\u5bfc\u9065\u611f\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u4f46\u5927\u591a\u6570\u65b9\u6cd5\u4ecd\u4f9d\u8d56\u989d\u5916\u53ef\u8bad\u7ec3\u7ec4\u4ef6\uff0c\u9650\u5236\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u5e94\u7528\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4ec5\u4f9d\u8d56\u73b0\u6709\u57fa\u7840\u6a21\u578b\u3001\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u57fa\u4e8e\u6587\u672c\u7684\u9065\u611f\u5206\u5272\u7684\u53ef\u80fd\u6027\u3002", "method": "\u63d0\u51fa\u7b80\u5355\u6709\u6548\u7684\u53cc\u8def\u5f84\u65b9\u6cd5\uff1a1\uff09\u5bf9\u6bd4\u5f0f\u65b9\u6cd5\u4f7f\u7528CLIP\u4f5c\u4e3aSAM\u7f51\u683c\u63d0\u6848\u7684\u63a9\u7801\u9009\u62e9\u5668\uff0c\u5b9e\u73b0\u5b8c\u5168\u96f6\u6837\u672c\u7684\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\uff1b2\uff09\u751f\u6210\u5f0f\u65b9\u6cd5\u4f7f\u7528GPT-5\uff08\u96f6\u6837\u672c\uff09\u548cLoRA\u8c03\u4f18\u7684Qwen-VL\u6a21\u578b\u4e3aSAM\u751f\u6210\u70b9\u51fb\u63d0\u793a\uff0c\u5b9e\u73b0\u63a8\u7406\u548c\u6307\u4ee3\u5206\u5272\u3002", "result": "\u572819\u4e2a\u9065\u611f\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec\u5f00\u653e\u8bcd\u6c47\u3001\u6307\u4ee3\u548c\u57fa\u4e8e\u63a8\u7406\u7684\u4efb\u52a1\uff09\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5bf9\u6bd4\u5f0f\u65b9\u6cd5\u5728\u5b8c\u5168\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\uff0c\u751f\u6210\u5f0f\u65b9\u6cd5\u4e2dLoRA\u8c03\u4f18\u7684Qwen-VL\u6a21\u578b\u53d6\u5f97\u4e86\u6700\u4f73\u7ed3\u679c\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u4ec5\u4f9d\u8d56\u73b0\u6709\u57fa\u7840\u6a21\u578b\u5373\u53ef\u5b9e\u73b0\u6709\u6548\u7684\u9065\u611f\u56fe\u50cf\u6587\u672c\u5f15\u5bfc\u5206\u5272\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u4ec5\u9700\u8f7b\u91cf\u8c03\u4f18\uff0c\u4e3a\u9065\u611f\u5206\u5272\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17814", "categories": ["cs.CV", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17814", "abs": "https://arxiv.org/abs/2602.17814", "authors": ["Adrian Catalin Lutu", "Eduard Poesina", "Radu Tudor Ionescu"], "title": "VQPP: Video Query Performance Prediction Benchmark", "comment": null, "summary": "Query performance prediction (QPP) is an important and actively studied information retrieval task, having various applications, such as query reformulation, query expansion, and retrieval system selection, among many others. The task has been primarily studied in the context of text and image retrieval, whereas QPP for content-based video retrieval (CBVR) remains largely underexplored. To this end, we propose the first benchmark for video query performance prediction (VQPP), comprising two text-to-video retrieval datasets and two CBVR systems, respectively. VQPP contains a total of 56K text queries and 51K videos, and comes with official training, validation and test splits, fostering direct comparisons and reproducible results. We explore multiple pre-retrieval and post-retrieval performance predictors, creating a representative benchmark for future exploration of QPP in the video domain. Our results show that pre-retrieval predictors obtain competitive performance, enabling applications before performing the retrieval step. We also demonstrate the applicability of VQPP by employing the best performing pre-retrieval predictor as reward model for training a large language model (LLM) on the query reformulation task via direct preference optimization (DPO). We release our benchmark and code at https://github.com/AdrianLutu/VQPP.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u89c6\u9891\u67e5\u8be2\u6027\u80fd\u9884\u6d4b\uff08VQPP\uff09\u57fa\u51c6\uff0c\u5305\u542b\u4e24\u4e2a\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u6570\u636e\u96c6\u548c\u4e24\u4e2aCBVR\u7cfb\u7edf\uff0c\u517156K\u6587\u672c\u67e5\u8be2\u548c51K\u89c6\u9891\uff0c\u63a2\u7d22\u4e86\u591a\u79cd\u9884\u68c0\u7d22\u548c\u540e\u68c0\u7d22\u6027\u80fd\u9884\u6d4b\u5668\uff0c\u5e76\u5c55\u793a\u4e86VQPP\u5728\u67e5\u8be2\u91cd\u5199\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u67e5\u8be2\u6027\u80fd\u9884\u6d4b\uff08QPP\uff09\u5728\u6587\u672c\u548c\u56fe\u50cf\u68c0\u7d22\u9886\u57df\u5df2\u6709\u6df1\u5165\u7814\u7a76\uff0c\u4f46\u5728\u57fa\u4e8e\u5185\u5bb9\u7684\u89c6\u9891\u68c0\u7d22\uff08CBVR\uff09\u4e2d\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\uff0c\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u89c6\u9891\u9886\u57df\u7684QPP\u57fa\u51c6\u3002", "method": "\u6784\u5efa\u4e86\u9996\u4e2a\u89c6\u9891\u67e5\u8be2\u6027\u80fd\u9884\u6d4b\u57fa\u51c6VQPP\uff0c\u5305\u542b\u4e24\u4e2a\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u6570\u636e\u96c6\u548c\u4e24\u4e2aCBVR\u7cfb\u7edf\u3002\u63a2\u7d22\u4e86\u591a\u79cd\u9884\u68c0\u7d22\u548c\u540e\u68c0\u7d22\u6027\u80fd\u9884\u6d4b\u5668\uff0c\u5e76\u4f7f\u7528\u6700\u4f73\u9884\u68c0\u7d22\u9884\u6d4b\u5668\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u67e5\u8be2\u91cd\u5199\u4efb\u52a1\u3002", "result": "\u9884\u68c0\u7d22\u9884\u6d4b\u5668\u83b7\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u80fd\u591f\u5728\u6267\u884c\u68c0\u7d22\u6b65\u9aa4\u4e4b\u524d\u5b9e\u73b0\u5e94\u7528\u3002VQPP\u57fa\u51c6\u5305\u542b56K\u6587\u672c\u67e5\u8be2\u548c51K\u89c6\u9891\uff0c\u63d0\u4f9b\u4e86\u5b98\u65b9\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u5212\u5206\uff0c\u4fc3\u8fdb\u4e86\u76f4\u63a5\u6bd4\u8f83\u548c\u53ef\u91cd\u590d\u7ed3\u679c\u3002", "conclusion": "VQPP\u662f\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\u89c6\u9891\u9886\u57df\u7684\u67e5\u8be2\u6027\u80fd\u9884\u6d4b\u57fa\u51c6\uff0c\u4e3a\u672a\u6765\u5728\u89c6\u9891\u9886\u57df\u63a2\u7d22QPP\u63d0\u4f9b\u4e86\u4ee3\u8868\u6027\u57fa\u51c6\u3002\u7814\u7a76\u5c55\u793a\u4e86\u9884\u68c0\u7d22\u9884\u6d4b\u5668\u7684\u6709\u6548\u6027\u53ca\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u67e5\u8be2\u91cd\u5199\u4efb\u52a1\u4e2d\u3002"}}
{"id": "2602.17854", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17854", "abs": "https://arxiv.org/abs/2602.17854", "authors": ["Domonkos Varga"], "title": "On the Evaluation Protocol of Gesture Recognition for UAV-based Rescue Operation based on Deep Learning: A Subject-Independence Perspective", "comment": null, "summary": "This paper presents a methodological analysis of the gesture-recognition approach proposed by Liu and Szir\u00e1nyi, with a particular focus on the validity of their evaluation protocol. We show that the reported near-perfect accuracy metrics result from a frame-level random train-test split that inevitably mixes samples from the same subjects across both sets, causing severe data leakage. By examining the published confusion matrix, learning curves, and dataset construction, we demonstrate that the evaluation does not measure generalization to unseen individuals. Our findings underscore the importance of subject-independent data partitioning in vision-based gesture-recognition research, especially for applications - such as UAV-human interaction - that require reliable recognition of gestures performed by previously unseen people.", "AI": {"tldr": "\u672c\u6587\u5bf9Liu\u548cSzir\u00e1nyi\u63d0\u51fa\u7684\u624b\u52bf\u8bc6\u522b\u65b9\u6cd5\u8fdb\u884c\u4e86\u65b9\u6cd5\u8bba\u5206\u6790\uff0c\u91cd\u70b9\u6307\u51fa\u5176\u8bc4\u4f30\u534f\u8bae\u5b58\u5728\u4e25\u91cd\u7684\u6570\u636e\u6cc4\u6f0f\u95ee\u9898\uff0c\u5bfc\u81f4\u62a5\u544a\u7684\u63a5\u8fd1\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\u6307\u6807\u4e0d\u53ef\u9760\u3002", "motivation": "\u8be5\u7814\u7a76\u7684\u52a8\u673a\u662f\u5206\u6790Liu\u548cSzir\u00e1nyi\u624b\u52bf\u8bc6\u522b\u65b9\u6cd5\u7684\u8bc4\u4f30\u534f\u8bae\u6709\u6548\u6027\uff0c\u63ed\u793a\u5176\u8bc4\u4f30\u4e2d\u5b58\u5728\u7684\u4e25\u91cd\u6570\u636e\u6cc4\u6f0f\u95ee\u9898\uff0c\u5f3a\u8c03\u5728\u57fa\u4e8e\u89c6\u89c9\u7684\u624b\u52bf\u8bc6\u522b\u7814\u7a76\u4e2d\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u65e0\u4eba\u673a-\u4eba\u4ea4\u4e92\u7b49\u9700\u8981\u53ef\u9760\u8bc6\u522b\u672a\u89c1\u4e2a\u4f53\u624b\u52bf\u7684\u5e94\u7528\uff0c\u72ec\u7acb\u4e8e\u53d7\u8bd5\u8005\u7684\u6570\u636e\u5212\u5206\u7684\u91cd\u8981\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5df2\u53d1\u8868\u7684\u6df7\u6dc6\u77e9\u9635\u3001\u5b66\u4e60\u66f2\u7ebf\u548c\u6570\u636e\u96c6\u6784\u5efa\u65b9\u5f0f\uff0c\u8bc1\u660e\u5176\u8bc4\u4f30\u534f\u8bae\u91c7\u7528\u5e27\u7ea7\u968f\u673a\u8bad\u7ec3-\u6d4b\u8bd5\u5206\u5272\uff0c\u4e0d\u53ef\u907f\u514d\u5730\u6df7\u5408\u4e86\u76f8\u540c\u53d7\u8bd5\u8005\u7684\u6837\u672c\uff0c\u5bfc\u81f4\u4e25\u91cd\u7684\u6570\u636e\u6cc4\u6f0f\uff0c\u65e0\u6cd5\u6d4b\u91cf\u5bf9\u672a\u89c1\u4e2a\u4f53\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u62a5\u544a\u7684\u63a5\u8fd1\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\u6307\u6807\u662f\u7531\u4e8e\u6570\u636e\u6cc4\u6f0f\u9020\u6210\u7684\uff0c\u8bc4\u4f30\u534f\u8bae\u672a\u80fd\u6d4b\u91cf\u5bf9\u672a\u89c1\u4e2a\u4f53\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u6df7\u6dc6\u77e9\u9635\u548c\u5b66\u4e60\u66f2\u7ebf\u5206\u6790\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u57fa\u4e8e\u89c6\u89c9\u7684\u624b\u52bf\u8bc6\u522b\u7814\u7a76\u4e2d\u91c7\u7528\u72ec\u7acb\u4e8e\u53d7\u8bd5\u8005\u7684\u6570\u636e\u5212\u5206\u7684\u91cd\u8981\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u9700\u8981\u53ef\u9760\u8bc6\u522b\u672a\u89c1\u4e2a\u4f53\u624b\u52bf\u7684\u5e94\u7528\u573a\u666f\uff0c\u5982\u65e0\u4eba\u673a-\u4eba\u4ea4\u4e92\u3002"}}
{"id": "2602.17869", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17869", "abs": "https://arxiv.org/abs/2602.17869", "authors": ["Yuxiao Chen", "Jue Wang", "Zhikang Zhang", "Jingru Yi", "Xu Zhang", "Yang Zou", "Zhaowei Cai", "Jianbo Yuan", "Xinyu Li", "Hao Yang", "Davide Modolo"], "title": "Learning Compact Video Representations for Efficient Long-form Video Understanding in Large Multimodal Models", "comment": null, "summary": "With recent advancements in video backbone architectures, combined with the remarkable achievements of large language models (LLMs), the analysis of long-form videos spanning tens of minutes has become both feasible and increasingly prevalent. However, the inherently redundant nature of video sequences poses significant challenges for contemporary state-of-the-art models. These challenges stem from two primary aspects: 1) efficiently incorporating a larger number of frames within memory constraints, and 2) extracting discriminative information from the vast volume of input data. In this paper, we introduce a novel end-to-end schema for long-form video understanding, which includes an information-density-based adaptive video sampler (AVS) and an autoencoder-based spatiotemporal video compressor (SVC) integrated with a multimodal large language model (MLLM). Our proposed system offers two major advantages: it adaptively and effectively captures essential information from video sequences of varying durations, and it achieves high compression rates while preserving crucial discriminative information. The proposed framework demonstrates promising performance across various benchmarks, excelling in both long-form video understanding tasks and standard video understanding benchmarks. These results underscore the versatility and efficacy of our approach, particularly in managing the complexities of prolonged video sequences.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u957f\u89c6\u9891\u7406\u89e3\u7684\u65b0\u6846\u67b6\uff0c\u5305\u542b\u81ea\u9002\u5e94\u89c6\u9891\u91c7\u6837\u5668\u548c\u65f6\u7a7a\u89c6\u9891\u538b\u7f29\u5668\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6709\u6548\u5904\u7406\u957f\u89c6\u9891\u4e2d\u7684\u5197\u4f59\u4fe1\u606f\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u89c6\u9891\u9aa8\u5e72\u67b6\u6784\u7684\u8fdb\u6b65\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u5206\u6790\u957f\u8fbe\u6570\u5341\u5206\u949f\u7684\u957f\u89c6\u9891\u53d8\u5f97\u53ef\u884c\u4e14\u666e\u904d\u3002\u7136\u800c\uff0c\u89c6\u9891\u5e8f\u5217\u56fa\u6709\u7684\u5197\u4f59\u6027\u7ed9\u73b0\u6709\u6a21\u578b\u5e26\u6765\u4e24\u5927\u6311\u6218\uff1a1) \u5728\u5185\u5b58\u9650\u5236\u5185\u6709\u6548\u5904\u7406\u66f4\u591a\u5e27\uff1b2) \u4ece\u5927\u91cf\u8f93\u5165\u6570\u636e\u4e2d\u63d0\u53d6\u5224\u522b\u6027\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u7aef\u5230\u7aef\u7684\u957f\u89c6\u9891\u7406\u89e3\u6846\u67b6\uff0c\u5305\u62ec\u57fa\u4e8e\u4fe1\u606f\u5bc6\u5ea6\u7684\u81ea\u9002\u5e94\u89c6\u9891\u91c7\u6837\u5668(AVS)\u548c\u57fa\u4e8e\u81ea\u52a8\u7f16\u7801\u5668\u7684\u65f6\u7a7a\u89c6\u9891\u538b\u7f29\u5668(SVC)\uff0c\u5e76\u4e0e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(MLLM)\u96c6\u6210\u3002", "result": "\u8be5\u6846\u67b6\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u65e2\u80fd\u9002\u5e94\u4e0d\u540c\u65f6\u957f\u7684\u89c6\u9891\u5e8f\u5217\u6709\u6548\u6355\u83b7\u5173\u952e\u4fe1\u606f\uff0c\u53c8\u80fd\u5b9e\u73b0\u9ad8\u538b\u7f29\u7387\u540c\u65f6\u4fdd\u7559\u91cd\u8981\u5224\u522b\u4fe1\u606f\uff0c\u5728\u957f\u89c6\u9891\u7406\u89e3\u548c\u6807\u51c6\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u5747\u6709\u4f18\u5f02\u8868\u73b0\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5904\u7406\u957f\u89c6\u9891\u5e8f\u5217\u590d\u6742\u6027\u65b9\u9762\u5c55\u73b0\u51fa\u591a\u529f\u80fd\u6027\u548c\u9ad8\u6548\u6027\uff0c\u4e3a\u89e3\u51b3\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5197\u4f59\u4fe1\u606f\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.17871", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.17871", "abs": "https://arxiv.org/abs/2602.17871", "authors": ["Dhruba Ghosh", "Yuhui Zhang", "Ludwig Schmidt"], "title": "Understanding the Fine-Grained Knowledge Capabilities of Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) have made substantial progress across a wide range of visual question answering benchmarks, spanning visual reasoning, document understanding, and multimodal dialogue. These improvements are evident in a wide range of VLMs built on a variety of base models, alignment architectures, and training data. However, recent works show that these models trail behind in traditional image classification benchmarks, which test fine-grained visual knowledge. We test a large number of recent VLMs on fine-grained classification benchmarks and identify potential factors in the disconnect between fine-grained knowledge and other vision benchmarks. Through a series of ablation experiments, we find that using a better LLM improves all benchmark scores equally, while a better vision encoder disproportionately improves fine-grained classification performance. Furthermore, we find that the pretraining stage is also vital to fine-grained performance, particularly when the language model weights are unfrozen during pretraining. These insights pave the way for enhancing fine-grained visual understanding and vision-centric capabilities in VLMs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u66f4\u597d\u7684\u89c6\u89c9\u7f16\u7801\u5668\u80fd\u663e\u8457\u63d0\u5347\u7ec6\u7c92\u5ea6\u5206\u7c7b\u6027\u80fd\uff0c\u800c\u9884\u8bad\u7ec3\u9636\u6bb5\uff08\u7279\u522b\u662f\u8bed\u8a00\u6a21\u578b\u6743\u91cd\u672a\u51bb\u7ed3\u65f6\uff09\u5bf9\u7ec6\u7c92\u5ea6\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5404\u79cd\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\u8fd9\u4e9b\u6a21\u578b\u5728\u4f20\u7edf\u7684\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u89c6\u89c9\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u4e0e\u5176\u4ed6\u89c6\u89c9\u57fa\u51c6\u4e4b\u95f4\u7684\u8131\u8282\u95ee\u9898\u3002", "method": "\u5bf9\u5927\u91cf\u8fd1\u671f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u5206\u7c7b\u57fa\u51c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u901a\u8fc7\u4e00\u7cfb\u5217\u6d88\u878d\u5b9e\u9a8c\u5206\u6790\u5f71\u54cd\u6027\u80fd\u7684\u56e0\u7d20\uff0c\u5305\u62ec\u4e0d\u540cLLM\u3001\u89c6\u89c9\u7f16\u7801\u5668\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u9884\u8bad\u7ec3\u9636\u6bb5\u8bed\u8a00\u6a21\u578b\u6743\u91cd\u51bb\u7ed3\u72b6\u6001\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u4f7f\u7528\u66f4\u597d\u7684LLM\u80fd\u540c\u7b49\u63d0\u5347\u6240\u6709\u57fa\u51c6\u5206\u6570\uff1b2\uff09\u66f4\u597d\u7684\u89c6\u89c9\u7f16\u7801\u5668\u80fd\u4e0d\u6210\u6bd4\u4f8b\u5730\u63d0\u5347\u7ec6\u7c92\u5ea6\u5206\u7c7b\u6027\u80fd\uff1b3\uff09\u9884\u8bad\u7ec3\u9636\u6bb5\u5bf9\u7ec6\u7c92\u5ea6\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5f53\u8bed\u8a00\u6a21\u578b\u6743\u91cd\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u672a\u51bb\u7ed3\u65f6\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u548c\u89c6\u89c9\u4e2d\u5fc3\u80fd\u529b\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u8868\u660e\u9700\u8981\u5173\u6ce8\u89c6\u89c9\u7f16\u7801\u5668\u8d28\u91cf\u548c\u9884\u8bad\u7ec3\u7b56\u7565\u6765\u63d0\u5347\u7ec6\u7c92\u5ea6\u89c6\u89c9\u77e5\u8bc6\u3002"}}
{"id": "2602.17909", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17909", "abs": "https://arxiv.org/abs/2602.17909", "authors": ["Amirhosein Javadi", "Chi-Shiang Gau", "Konstantinos D. Polyzos", "Tara Javidi"], "title": "A Single Image and Multimodality Is All You Need for Novel View Synthesis", "comment": null, "summary": "Diffusion-based approaches have recently demonstrated strong performance for single-image novel view synthesis by conditioning generative models on geometry inferred from monocular depth estimation. However, in practice, the quality and consistency of the synthesized views are fundamentally limited by the reliability of the underlying depth estimates, which are often fragile under low texture, adverse weather, and occlusion-heavy real-world conditions. In this work, we show that incorporating sparse multimodal range measurements provides a simple yet effective way to overcome these limitations. We introduce a multimodal depth reconstruction framework that leverages extremely sparse range sensing data, such as automotive radar or LiDAR, to produce dense depth maps that serve as robust geometric conditioning for diffusion-based novel view synthesis. Our approach models depth in an angular domain using a localized Gaussian Process formulation, enabling computationally efficient inference while explicitly quantifying uncertainty in regions with limited observations. The reconstructed depth and uncertainty are used as a drop-in replacement for monocular depth estimators in existing diffusion-based rendering pipelines, without modifying the generative model itself. Experiments on real-world multimodal driving scenes demonstrate that replacing vision-only depth with our sparse range-based reconstruction substantially improves both geometric consistency and visual quality in single-image novel-view video generation. These results highlight the importance of reliable geometric priors for diffusion-based view synthesis and demonstrate the practical benefits of multimodal sensing even at extreme levels of sparsity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u7a00\u758f\u591a\u6a21\u6001\u6d4b\u8ddd\u6570\u636e\uff08\u5982\u96f7\u8fbe\u6216\u6fc0\u5149\u96f7\u8fbe\uff09\u6765\u6539\u8fdb\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5355\u56fe\u50cf\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u66f4\u53ef\u9760\u7684\u51e0\u4f55\u5148\u9a8c\u6765\u89e3\u51b3\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5355\u56fe\u50cf\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u63d0\u4f9b\u7684\u51e0\u4f55\u4fe1\u606f\uff0c\u4f46\u5728\u4f4e\u7eb9\u7406\u3001\u6076\u52a3\u5929\u6c14\u548c\u906e\u6321\u4e25\u91cd\u7684\u771f\u5b9e\u573a\u666f\u4e2d\uff0c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7684\u53ef\u9760\u6027\u6709\u9650\uff0c\u5bfc\u81f4\u5408\u6210\u89c6\u56fe\u7684\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u53d7\u5230\u5236\u7ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6df1\u5ea6\u91cd\u5efa\u6846\u67b6\uff0c\u5229\u7528\u6781\u7a00\u758f\u7684\u6d4b\u8ddd\u4f20\u611f\u6570\u636e\uff08\u5982\u6c7d\u8f66\u96f7\u8fbe\u6216\u6fc0\u5149\u96f7\u8fbe\uff09\u751f\u6210\u5bc6\u96c6\u6df1\u5ea6\u56fe\u3002\u91c7\u7528\u57fa\u4e8e\u89d2\u57df\u7684\u5c40\u90e8\u9ad8\u65af\u8fc7\u7a0b\u5efa\u6a21\u65b9\u6cd5\uff0c\u5b9e\u73b0\u8ba1\u7b97\u9ad8\u6548\u63a8\u7406\u5e76\u663e\u5f0f\u91cf\u5316\u89c2\u6d4b\u6709\u9650\u533a\u57df\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u91cd\u5efa\u7684\u6df1\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\u53ef\u76f4\u63a5\u66ff\u4ee3\u73b0\u6709\u6269\u6563\u6e32\u67d3\u6d41\u7a0b\u4e2d\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5668\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u591a\u6a21\u6001\u9a7e\u9a76\u573a\u666f\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7528\u7a00\u758f\u6d4b\u8ddd\u91cd\u5efa\u6df1\u5ea6\u66ff\u4ee3\u7eaf\u89c6\u89c9\u6df1\u5ea6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5355\u56fe\u50cf\u65b0\u89c6\u89d2\u89c6\u9891\u751f\u6210\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u53ef\u9760\u51e0\u4f55\u5148\u9a8c\u5bf9\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u89d2\u5408\u6210\u7684\u91cd\u8981\u6027\uff0c\u5e76\u8bc1\u660e\u4e86\u5373\u4f7f\u5728\u6781\u7aef\u7a00\u758f\u6761\u4ef6\u4e0b\uff0c\u591a\u6a21\u6001\u4f20\u611f\u4e5f\u80fd\u5e26\u6765\u5b9e\u9645\u6548\u76ca\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u4fee\u6539\u751f\u6210\u6a21\u578b\u672c\u8eab\uff0c\u53ef\u4f5c\u4e3a\u73b0\u6709\u6269\u6563\u6e32\u67d3\u6d41\u7a0b\u7684\u5373\u63d2\u5373\u7528\u6539\u8fdb\u65b9\u6848\u3002"}}
{"id": "2602.17929", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.17929", "abs": "https://arxiv.org/abs/2602.17929", "authors": ["Athanasios Angelakis"], "title": "ZACH-ViT: Regime-Dependent Inductive Bias in Compact Vision Transformers for Medical Imaging", "comment": "15 pages, 12 figures, 7 tables. Code and models available at https://github.com/Bluesman79/ZACH-ViT", "summary": "Vision Transformers rely on positional embeddings and class tokens that encode fixed spatial priors. While effective for natural images, these priors may hinder generalization when spatial layout is weakly informative or inconsistent, a frequent condition in medical imaging and edge-deployed clinical systems. We introduce ZACH-ViT (Zero-token Adaptive Compact Hierarchical Vision Transformer), a compact Vision Transformer that removes both positional embeddings and the [CLS] token, achieving permutation invariance through global average pooling over patch representations. The term \"Zero-token\" specifically refers to removing the dedicated [CLS] aggregation token and positional embeddings; patch tokens remain unchanged and are processed normally. Adaptive residual projections preserve training stability in compact configurations while maintaining a strict parameter budget.\n  Evaluation is performed across seven MedMNIST datasets spanning binary and multi-class tasks under a strict few-shot protocol (50 samples per class, fixed hyperparameters, five random seeds). The empirical analysis demonstrates regime-dependent behavior: ZACH-ViT (0.25M parameters, trained from scratch) achieves its strongest advantage on BloodMNIST and remains competitive with TransMIL on PathMNIST, while its relative advantage decreases on datasets with strong anatomical priors (OCTMNIST, OrganAMNIST), consistent with the architectural hypothesis. These findings support the view that aligning architectural inductive bias with data structure can be more important than pursuing universal benchmark dominance. Despite its minimal size and lack of pretraining, ZACH-ViT achieves competitive performance while maintaining sub-second inference times, supporting deployment in resource-constrained clinical environments. Code and models are available at https://github.com/Bluesman79/ZACH-ViT.", "AI": {"tldr": "ZACH-ViT\u662f\u4e00\u79cd\u7d27\u51d1\u7684\u89c6\u89c9Transformer\uff0c\u79fb\u9664\u4e86\u4f4d\u7f6e\u5d4c\u5165\u548cCLS\u6807\u8bb0\uff0c\u901a\u8fc7\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5b9e\u73b0\u6392\u5217\u4e0d\u53d8\u6027\uff0c\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u4e34\u5e8a\u73af\u5883\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9Transformer\u4f9d\u8d56\u4f4d\u7f6e\u5d4c\u5165\u548c\u7c7b\u522b\u6807\u8bb0\u7f16\u7801\u56fa\u5b9a\u7684\u7a7a\u95f4\u5148\u9a8c\uff0c\u8fd9\u5728\u81ea\u7136\u56fe\u50cf\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u533b\u5b66\u6210\u50cf\u4e2d\u53ef\u80fd\u963b\u788d\u6cdb\u5316\u80fd\u529b\uff0c\u56e0\u4e3a\u533b\u5b66\u56fe\u50cf\u7684\u7a7a\u95f4\u5e03\u5c40\u4fe1\u606f\u8f83\u5f31\u6216\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51faZACH-ViT\uff08Zero-token Adaptive Compact Hierarchical Vision Transformer\uff09\uff0c\u79fb\u9664\u4f4d\u7f6e\u5d4c\u5165\u548cCLS\u6807\u8bb0\uff0c\u901a\u8fc7\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5b9e\u73b0\u6392\u5217\u4e0d\u53d8\u6027\uff0c\u4f7f\u7528\u81ea\u9002\u5e94\u6b8b\u5dee\u6295\u5f71\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u540c\u65f6\u4e25\u683c\u63a7\u5236\u53c2\u6570\u9884\u7b97\u3002", "result": "\u57287\u4e2aMedMNIST\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff1aZACH-ViT\uff080.25M\u53c2\u6570\uff09\u5728BloodMNIST\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u5728PathMNIST\u4e0a\u4e0eTransMIL\u7ade\u4e89\uff0c\u4f46\u5728\u5177\u6709\u5f3a\u89e3\u5256\u5148\u9a8c\u7684\u6570\u636e\u96c6\uff08OCTMNIST\u3001OrganAMNIST\uff09\u4e0a\u76f8\u5bf9\u4f18\u52bf\u51cf\u5f31\uff0c\u7b26\u5408\u67b6\u6784\u5047\u8bbe\u3002", "conclusion": "\u5c06\u67b6\u6784\u5f52\u7eb3\u504f\u7f6e\u4e0e\u6570\u636e\u7ed3\u6784\u5bf9\u9f50\u6bd4\u8ffd\u6c42\u901a\u7528\u57fa\u51c6\u4f18\u52bf\u66f4\u91cd\u8981\u3002\u5c3d\u7ba1\u5c3a\u5bf8\u6781\u5c0f\u4e14\u65e0\u9884\u8bad\u7ec3\uff0cZACH-ViT\u5728\u8d44\u6e90\u53d7\u9650\u7684\u4e34\u5e8a\u73af\u5883\u4e2d\u4ecd\u80fd\u4fdd\u6301\u4e9a\u79d2\u7ea7\u63a8\u7406\u65f6\u95f4\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2602.17951", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17951", "abs": "https://arxiv.org/abs/2602.17951", "authors": ["Guoheng Sun", "Tingting Du", "Kaixi Feng", "Chenxiang Luo", "Xingguo Ding", "Zheyu Shen", "Ziyao Wang", "Yexiao He", "Ang Li"], "title": "ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action (VLA) models enable instruction-following robotic manipulation, but they are typically pretrained on 2D data and lack 3D spatial understanding. An effective approach is representation alignment, where a strong vision foundation model is used to guide a 2D VLA model. However, existing methods usually apply supervision at only a single layer, failing to fully exploit the rich information distributed across depth; meanwhile, na\u00efve multi-layer alignment can cause gradient interference. We introduce ROCKET, a residual-oriented multi-layer representation alignment framework that formulates multi-layer alignment as aligning one residual stream to another. Concretely, ROCKET employs a shared projector to align multiple layers of the VLA backbone with multiple layers of a powerful 3D vision foundation model via a layer-invariant mapping, which reduces gradient conflicts. We provide both theoretical justification and empirical analyses showing that a shared projector is sufficient and outperforms prior designs, and further propose a Matryoshka-style sparse activation scheme for the shared projector to balance multiple alignment losses. Our experiments show that, combined with a training-free layer selection strategy, ROCKET requires only about 4% of the compute budget while achieving 98.5% state-of-the-art success rate on LIBERO. We further demonstrate the superior performance of ROCKET across LIBERO-Plus and RoboTwin, as well as multiple VLA models. The code and model weights can be found at https://github.com/CASE-Lab-UMD/ROCKET-VLA.", "AI": {"tldr": "ROCKET\u63d0\u51fa\u4e86\u4e00\u79cd\u6b8b\u5dee\u5bfc\u5411\u7684\u591a\u5c42\u8868\u793a\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u6295\u5f71\u5668\u548c\u5c42\u4e0d\u53d8\u6620\u5c04\u5c06VLA\u6a21\u578b\u7684\u591a\u4e2a\u5c42\u4e0e3D\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u53473D\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684Vision-Language-Action\u6a21\u578b\u901a\u5e38\u57282D\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\uff0c\u7f3a\u4e4f3D\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u3002\u73b0\u6709\u7684\u8868\u793a\u5bf9\u9f50\u65b9\u6cd5\u901a\u5e38\u53ea\u5728\u5355\u5c42\u8fdb\u884c\u76d1\u7763\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u6df1\u5ea6\u5206\u5e03\u4e2d\u7684\u4e30\u5bcc\u4fe1\u606f\uff0c\u800c\u7b80\u5355\u7684\u591a\u5c42\u5bf9\u9f50\u53ef\u80fd\u5bfc\u81f4\u68af\u5ea6\u5e72\u6270\u3002", "method": "\u63d0\u51faROCKET\u6846\u67b6\uff0c\u5c06\u591a\u5c42\u5bf9\u9f50\u5efa\u6a21\u4e3a\u5c06\u4e00\u4e2a\u6b8b\u5dee\u6d41\u4e0e\u53e6\u4e00\u4e2a\u6b8b\u5dee\u6d41\u5bf9\u9f50\u3002\u4f7f\u7528\u5171\u4eab\u6295\u5f71\u5668\u901a\u8fc7\u5c42\u4e0d\u53d8\u6620\u5c04\u5c06VLA\u9aa8\u5e72\u7684\u591a\u4e2a\u5c42\u4e0e\u5f3a\u5927\u76843D\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u591a\u4e2a\u5c42\u5bf9\u9f50\uff0c\u51cf\u5c11\u68af\u5ea6\u51b2\u7a81\u3002\u8fdb\u4e00\u6b65\u63d0\u51faMatryoshka\u98ce\u683c\u7684\u7a00\u758f\u6fc0\u6d3b\u65b9\u6848\u6765\u5e73\u8861\u591a\u4e2a\u5bf9\u9f50\u635f\u5931\uff0c\u5e76\u7ed3\u5408\u514d\u8bad\u7ec3\u5c42\u9009\u62e9\u7b56\u7565\u3002", "result": "ROCKET\u4ec5\u9700\u7ea64%\u7684\u8ba1\u7b97\u9884\u7b97\uff0c\u5728LIBERO\u4e0a\u8fbe\u523098.5%\u7684\u6700\u5148\u8fdb\u6210\u529f\u7387\u3002\u5728LIBERO-Plus\u548cRoboTwin\u4ee5\u53ca\u591a\u4e2aVLA\u6a21\u578b\u4e0a\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "ROCKET\u901a\u8fc7\u6b8b\u5dee\u5bfc\u5411\u7684\u591a\u5c42\u8868\u793a\u5bf9\u9f50\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709VLA\u6a21\u578b\u7f3a\u4e4f3D\u7a7a\u95f4\u7406\u89e3\u7684\u95ee\u9898\uff0c\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2602.18000", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18000", "abs": "https://arxiv.org/abs/2602.18000", "authors": ["Xuting Lan", "Mingliang Zhou", "Xuekai Wei", "Jielu Yan", "Yueting Huang", "Huayan Pu", "Jun Luo", "Weijia Jia"], "title": "Image Quality Assessment: Exploring Quality Awareness via Memory-driven Distortion Patterns Matching", "comment": null, "summary": "Existing full-reference image quality assessment (FR-IQA) methods achieve high-precision evaluation by analysing feature differences between reference and distorted images. However, their performance is constrained by the quality of the reference image, which limits real-world applications where ideal reference sources are unavailable. Notably, the human visual system has the ability to accumulate visual memory, allowing image quality assessment on the basis of long-term memory storage. Inspired by this biological memory mechanism, we propose a memory-driven quality-aware framework (MQAF), which establishes a memory bank for storing distortion patterns and dynamically switches between dual-mode quality assessment strategies to reduce reliance on high-quality reference images. When reference images are available, MQAF obtains reference-guided quality scores by adaptively weighting reference information and comparing the distorted image with stored distortion patterns in the memory bank. When the reference image is absent, the framework relies on distortion patterns in the memory bank to infer image quality, enabling no-reference quality assessment (NR-IQA). The experimental results show that our method outperforms state-of-the-art approaches across multiple datasets while adapting to both no-reference and full-reference tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bb0\u5fc6\u9a71\u52a8\u7684\u8d28\u91cf\u611f\u77e5\u6846\u67b6\uff08MQAF\uff09\uff0c\u901a\u8fc7\u5efa\u7acb\u5b58\u50a8\u5931\u771f\u6a21\u5f0f\u7684\u8bb0\u5fc6\u5e93\uff0c\u5728\u6709\u65e0\u53c2\u8003\u56fe\u50cf\u7684\u60c5\u51b5\u4e0b\u90fd\u80fd\u8fdb\u884c\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff0c\u51cf\u5c11\u5bf9\u9ad8\u8d28\u91cf\u53c2\u8003\u56fe\u50cf\u7684\u4f9d\u8d56\u3002", "motivation": "\u73b0\u6709\u5168\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08FR-IQA\uff09\u65b9\u6cd5\u4f9d\u8d56\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\uff0c\u9650\u5236\u4e86\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u53d7\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u80fd\u591f\u79ef\u7d2f\u89c6\u89c9\u8bb0\u5fc6\u7684\u542f\u53d1\uff0c\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u51cf\u5c11\u5bf9\u9ad8\u8d28\u91cf\u53c2\u8003\u56fe\u50cf\u4f9d\u8d56\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u8bb0\u5fc6\u9a71\u52a8\u7684\u8d28\u91cf\u611f\u77e5\u6846\u67b6\uff08MQAF\uff09\uff0c\u5efa\u7acb\u5b58\u50a8\u5931\u771f\u6a21\u5f0f\u7684\u8bb0\u5fc6\u5e93\uff0c\u91c7\u7528\u53cc\u6a21\u5f0f\u8d28\u91cf\u8bc4\u4f30\u7b56\u7565\uff1a\u6709\u53c2\u8003\u56fe\u50cf\u65f6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u52a0\u6743\u53c2\u8003\u4fe1\u606f\u5e76\u4e0e\u8bb0\u5fc6\u5e93\u4e2d\u7684\u5931\u771f\u6a21\u5f0f\u6bd4\u8f83\u83b7\u5f97\u8d28\u91cf\u5206\u6570\uff1b\u65e0\u53c2\u8003\u56fe\u50cf\u65f6\uff0c\u4f9d\u8d56\u8bb0\u5fc6\u5e93\u4e2d\u7684\u5931\u771f\u6a21\u5f0f\u63a8\u65ad\u56fe\u50cf\u8d28\u91cf\uff0c\u5b9e\u73b0\u65e0\u53c2\u8003\u8d28\u91cf\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u80fd\u591f\u9002\u5e94\u65e0\u53c2\u8003\u548c\u5168\u53c2\u8003\u4e24\u79cd\u4efb\u52a1\u3002", "conclusion": "MQAF\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u89c6\u89c9\u8bb0\u5fc6\u673a\u5236\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u5bf9\u9ad8\u8d28\u91cf\u53c2\u8003\u56fe\u50cf\u7684\u4f9d\u8d56\uff0c\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u5177\u6709\u66f4\u597d\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2602.18006", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18006", "abs": "https://arxiv.org/abs/2602.18006", "authors": ["Ahsan Baidar Bakht", "Mohamad Alansari", "Muhayy Ud Din", "Muzammal Naseer", "Sajid Javed", "Irfan Hussain", "Jiri Matas", "Arif Mahmood"], "title": "MUOT_3M: A 3 Million Frame Multimodal Underwater Benchmark and the MUTrack Tracking Method", "comment": null, "summary": "Underwater Object Tracking (UOT) is crucial for efficient marine robotics, large scale ecological monitoring, and ocean exploration; however, progress has been hindered by the scarcity of large, multimodal, and diverse datasets. Existing benchmarks remain small and RGB only, limiting robustness under severe color distortion, turbidity, and low visibility conditions. We introduce MUOT_3M, the first pseudo multimodal UOT benchmark comprising 3 million frames from 3,030 videos (27.8h) annotated with 32 tracking attributes, 677 fine grained classes, and synchronized RGB, estimated enhanced RGB, estimated depth, and language modalities validated by a marine biologist. Building upon MUOT_3M, we propose MUTrack, a SAM-based multimodal to unimodal tracker featuring visual geometric alignment, vision language fusion, and four level knowledge distillation that transfers multimodal knowledge into a unimodal student model. Extensive evaluations across five UOT benchmarks demonstrate that MUTrack achieves up to 8.40% higher AUC and 7.80% higher precision than the strongest SOTA baselines while running at 24 FPS. MUOT_3M and MUTrack establish a new foundation for scalable, multimodally trained yet practically deployable underwater tracking.", "AI": {"tldr": "MUOT_3M\u662f\u9996\u4e2a\u4f2a\u591a\u6a21\u6001\u6c34\u4e0b\u76ee\u6807\u8ddf\u8e2a\u57fa\u51c6\uff0c\u5305\u542b300\u4e07\u5e27\u89c6\u9891\u6570\u636e\uff0c\u652f\u6301RGB\u3001\u589e\u5f3aRGB\u3001\u6df1\u5ea6\u548c\u8bed\u8a00\u56db\u79cd\u6a21\u6001\u3002\u57fa\u4e8e\u6b64\u57fa\u51c6\u63d0\u51fa\u7684MUTrack\u591a\u6a21\u6001\u5230\u5355\u6a21\u6001\u8ddf\u8e2a\u5668\uff0c\u901a\u8fc7\u89c6\u89c9\u51e0\u4f55\u5bf9\u9f50\u3001\u89c6\u89c9\u8bed\u8a00\u878d\u5408\u548c\u56db\u7ea7\u77e5\u8bc6\u84b8\u998f\uff0c\u5728\u4e94\u4e2aUOT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6c34\u4e0b\u76ee\u6807\u8ddf\u8e2a\u5bf9\u6d77\u6d0b\u673a\u5668\u4eba\u3001\u751f\u6001\u76d1\u6d4b\u548c\u6d77\u6d0b\u63a2\u7d22\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u4e14\u4ec5\u652f\u6301RGB\u6a21\u6001\uff0c\u9650\u5236\u4e86\u5728\u989c\u8272\u5931\u771f\u3001\u6d51\u6d4a\u548c\u4f4e\u80fd\u89c1\u5ea6\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u9700\u8981\u5927\u89c4\u6a21\u3001\u591a\u6a21\u6001\u3001\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u6765\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u3002", "method": "1) \u6784\u5efaMUOT_3M\u57fa\u51c6\uff1a\u5305\u542b3,030\u4e2a\u89c6\u9891\u3001300\u4e07\u5e27\uff0c\u6807\u6ce832\u4e2a\u8ddf\u8e2a\u5c5e\u6027\u3001677\u4e2a\u7ec6\u7c92\u5ea6\u7c7b\u522b\uff0c\u63d0\u4f9bRGB\u3001\u4f30\u8ba1\u589e\u5f3aRGB\u3001\u4f30\u8ba1\u6df1\u5ea6\u548c\u8bed\u8a00\u56db\u79cd\u540c\u6b65\u6a21\u6001\uff1b2) \u63d0\u51faMUTrack\u8ddf\u8e2a\u5668\uff1a\u57fa\u4e8eSAM\u67b6\u6784\uff0c\u91c7\u7528\u89c6\u89c9\u51e0\u4f55\u5bf9\u9f50\u3001\u89c6\u89c9\u8bed\u8a00\u878d\u5408\u548c\u56db\u7ea7\u77e5\u8bc6\u84b8\u998f\uff0c\u5c06\u591a\u6a21\u6001\u77e5\u8bc6\u8fc1\u79fb\u5230\u5355\u6a21\u6001\u5b66\u751f\u6a21\u578b\u4e2d\u3002", "result": "\u5728\u4e94\u4e2a\u6c34\u4e0b\u76ee\u6807\u8ddf\u8e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMUTrack\u76f8\u6bd4\u6700\u5f3aSOTA\u57fa\u7ebf\uff0cAUC\u63d0\u5347\u9ad8\u8fbe8.40%\uff0c\u7cbe\u5ea6\u63d0\u5347\u9ad8\u8fbe7.80%\uff0c\u540c\u65f6\u8fd0\u884c\u901f\u5ea6\u8fbe\u523024 FPS\u3002MUOT_3M\u57fa\u51c6\u7ecf\u8fc7\u6d77\u6d0b\u751f\u7269\u5b66\u5bb6\u9a8c\u8bc1\u3002", "conclusion": "MUOT_3M\u548cMUTrack\u4e3a\u53ef\u6269\u5c55\u3001\u591a\u6a21\u6001\u8bad\u7ec3\u4f46\u5b9e\u9645\u53ef\u90e8\u7f72\u7684\u6c34\u4e0b\u8ddf\u8e2a\u5efa\u7acb\u4e86\u65b0\u57fa\u7840\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u89c4\u6a21\u5c0f\u3001\u6a21\u6001\u5355\u4e00\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u4e0b\u76ee\u6807\u8ddf\u8e2a\u7684\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2602.18016", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18016", "abs": "https://arxiv.org/abs/2602.18016", "authors": ["Jiamin Luo", "Xuqian Gu", "Jingjing Wang", "Jiahong Lu"], "title": "Towards LLM-centric Affective Visual Customization via Efficient and Precise Emotion Manipulating", "comment": null, "summary": "Previous studies on visual customization primarily rely on the objective alignment between various control signals (e.g., language, layout and canny) and the edited images, which largely ignore the subjective emotional contents, and more importantly lack general-purpose foundation models for affective visual customization. With this in mind, this paper proposes an LLM-centric Affective Visual Customization (L-AVC) task, which focuses on generating images within modifying their subjective emotions via Multimodal LLM. Further, this paper contends that how to make the model efficiently align emotion conversion in semantics (named inter-emotion semantic conversion) and how to precisely retain emotion-agnostic contents (named exter-emotion semantic retaining) are rather important and challenging in this L-AVC task. To this end, this paper proposes an Efficient and Precise Emotion Manipulating approach for editing subjective emotions in images. Specifically, an Efficient Inter-emotion Converting (EIC) module is tailored to make the LLM efficiently align emotion conversion in semantics before and after editing, followed by a Precise Exter-emotion Retaining (PER) module to precisely retain the emotion-agnostic contents. Comprehensive experimental evaluations on our constructed L-AVC dataset demonstrate the great advantage of the proposed EPEM approach to the L-AVC task over several state-of-the-art baselines. This justifies the importance of emotion information for L-AVC and the effectiveness of EPEM in efficiently and precisely manipulating such information.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684LLM\u4e2d\u5fc3\u60c5\u611f\u89c6\u89c9\u5b9a\u5236\uff08L-AVC\uff09\u4efb\u52a1\uff0c\u65e8\u5728\u901a\u8fc7\u7f16\u8f91\u56fe\u50cf\u7684\u4e3b\u89c2\u60c5\u611f\u5185\u5bb9\u6765\u751f\u6210\u56fe\u50cf\uff0c\u5e76\u63d0\u51fa\u4e86\u9ad8\u6548\u7cbe\u786e\u7684\u60c5\u611f\u64cd\u7eb5\u65b9\u6cd5\uff08EPEM\uff09\u6765\u89e3\u51b3\u60c5\u611f\u8bed\u4e49\u8f6c\u6362\u548c\u60c5\u611f\u65e0\u5173\u5185\u5bb9\u4fdd\u7559\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u5b9a\u5236\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u5404\u79cd\u63a7\u5236\u4fe1\u53f7\uff08\u5982\u8bed\u8a00\u3001\u5e03\u5c40\u3001\u8fb9\u7f18\u68c0\u6d4b\uff09\u4e0e\u7f16\u8f91\u56fe\u50cf\u4e4b\u95f4\u7684\u5ba2\u89c2\u5bf9\u9f50\uff0c\u5ffd\u7565\u4e86\u4e3b\u89c2\u60c5\u611f\u5185\u5bb9\uff0c\u5e76\u4e14\u7f3a\u4e4f\u7528\u4e8e\u60c5\u611f\u89c6\u89c9\u5b9a\u5236\u7684\u901a\u7528\u57fa\u7840\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u9ad8\u6548\u7cbe\u786e\u7684\u60c5\u611f\u64cd\u7eb5\u65b9\u6cd5\uff08EPEM\uff09\uff0c\u5305\u62ec\u9ad8\u6548\u60c5\u611f\u95f4\u8f6c\u6362\uff08EIC\uff09\u6a21\u5757\u4f7fLLM\u5728\u7f16\u8f91\u524d\u540e\u9ad8\u6548\u5bf9\u9f50\u60c5\u611f\u8bed\u4e49\u8f6c\u6362\uff0c\u4ee5\u53ca\u7cbe\u786e\u60c5\u611f\u5916\u4fdd\u7559\uff08PER\uff09\u6a21\u5757\u7cbe\u786e\u4fdd\u7559\u60c5\u611f\u65e0\u5173\u5185\u5bb9\u3002", "result": "\u5728\u6784\u5efa\u7684L-AVC\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684EPEM\u65b9\u6cd5\u5728L-AVC\u4efb\u52a1\u4e0a\u4f18\u4e8e\u591a\u4e2a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u60c5\u611f\u4fe1\u606f\u5bf9L-AVC\u7684\u91cd\u8981\u6027\u4ee5\u53caEPEM\u5728\u9ad8\u6548\u7cbe\u786e\u64cd\u7eb5\u6b64\u7c7b\u4fe1\u606f\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684L-AVC\u4efb\u52a1\u548cEPEM\u65b9\u6cd5\u586b\u8865\u4e86\u60c5\u611f\u89c6\u89c9\u5b9a\u5236\u9886\u57df\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u591a\u6a21\u6001LLM\u5b9e\u73b0\u4e86\u5bf9\u56fe\u50cf\u4e3b\u89c2\u60c5\u611f\u7684\u9ad8\u6548\u7cbe\u786e\u7f16\u8f91\uff0c\u4e3a\u60c5\u611f\u611f\u77e5\u7684\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2602.18019", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18019", "abs": "https://arxiv.org/abs/2602.18019", "authors": ["Yujie Jin", "Wenxin Zhang", "Jingjing Wang", "Guodong Zhou"], "title": "DeepSVU: Towards In-depth Security-oriented Video Understanding via Unified Physical-world Regularized MoE", "comment": null, "summary": "In the literature, prior research on Security-oriented Video Understanding (SVU) has predominantly focused on detecting and localize the threats (e.g., shootings, robberies) in videos, while largely lacking the effective capability to generate and evaluate the threat causes. Motivated by these gaps, this paper introduces a new chat paradigm SVU task, i.e., In-depth Security-oriented Video Understanding (DeepSVU), which aims to not only identify and locate the threats but also attribute and evaluate the causes threatening segments. Furthermore, this paper reveals two key challenges in the proposed task: 1) how to effectively model the coarse-to-fine physical-world information (e.g., human behavior, object interactions and background context) to boost the DeepSVU task; and 2) how to adaptively trade off these factors. To tackle these challenges, this paper proposes a new Unified Physical-world Regularized MoE (UPRM) approach. Specifically, UPRM incorporates two key components: the Unified Physical-world Enhanced MoE (UPE) Block and the Physical-world Trade-off Regularizer (PTR), to address the above two challenges, respectively. Extensive experiments conduct on our DeepSVU instructions datasets (i.e., UCF-C instructions and CUVA instructions) demonstrate that UPRM outperforms several advanced Video-LLMs as well as non-VLM approaches. Such information.These justify the importance of the coarse-to-fine physical-world information in the DeepSVU task and demonstrate the effectiveness of our UPRM in capturing such information.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u6df1\u5ea6\u5b89\u5168\u5bfc\u5411\u89c6\u9891\u7406\u89e3\u4efb\u52a1DeepSVU\uff0c\u65e8\u5728\u4e0d\u4ec5\u8bc6\u522b\u548c\u5b9a\u4f4d\u5a01\u80c1\uff0c\u8fd8\u8981\u5f52\u56e0\u548c\u8bc4\u4f30\u5a01\u80c1\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u7edf\u4e00\u7269\u7406\u4e16\u754c\u6b63\u5219\u5316MoE\u65b9\u6cd5\u6765\u89e3\u51b3\u8be5\u4efb\u52a1\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u5bfc\u5411\u89c6\u9891\u7406\u89e3\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u68c0\u6d4b\u548c\u5b9a\u4f4d\u5a01\u80c1\uff0c\u4f46\u7f3a\u4e4f\u751f\u6210\u548c\u8bc4\u4f30\u5a01\u80c1\u539f\u56e0\u7684\u6709\u6548\u80fd\u529b\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u6df1\u5ea6\u5b89\u5168\u5bfc\u5411\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86\u7edf\u4e00\u7269\u7406\u4e16\u754c\u6b63\u5219\u5316MoE\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u7edf\u4e00\u7269\u7406\u4e16\u754c\u589e\u5f3aMoE\u5757\u548c\u7269\u7406\u4e16\u754c\u6743\u8861\u6b63\u5219\u5316\u5668\uff0c\u5206\u522b\u89e3\u51b3\u7c97\u5230\u7ec6\u7269\u7406\u4e16\u754c\u4fe1\u606f\u5efa\u6a21\u548c\u81ea\u9002\u5e94\u6743\u8861\u8fd9\u4e9b\u56e0\u7d20\u7684\u6311\u6218\u3002", "result": "\u5728DeepSVU\u6307\u4ee4\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUPRM\u65b9\u6cd5\u4f18\u4e8e\u591a\u4e2a\u5148\u8fdb\u7684\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u548c\u975eVLM\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u7c97\u5230\u7ec6\u7269\u7406\u4e16\u754c\u4fe1\u606f\u5728DeepSVU\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u7c97\u5230\u7ec6\u7269\u7406\u4e16\u754c\u4fe1\u606f\u5728\u6df1\u5ea6\u5b89\u5168\u5bfc\u5411\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5c55\u793a\u4e86UPRM\u65b9\u6cd5\u5728\u6355\u6349\u6b64\u7c7b\u4fe1\u606f\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.18020", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18020", "abs": "https://arxiv.org/abs/2602.18020", "authors": ["Jiabing Yang", "Yixiang Chen", "Yuan Xu", "Peiyan Li", "Xiangnan Wu", "Zichen Wen", "Bowen Fang", "Tao Yu", "Zhengbo Zhang", "Yingda Li", "Kai Wang", "Jing Liu", "Nianfeng Liu", "Yan Huang", "Liang Wang"], "title": "UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action (VLA) models leverage pretrained Vision-Language Models (VLMs) as backbones to map images and instructions to actions, demonstrating remarkable potential for generalizable robotic manipulation. To enhance performance, existing methods often incorporate extra observation cues (e.g., depth maps, point clouds) or auxiliary modules (e.g., object detectors, encoders) to enable more precise and reliable task execution, yet these typically require costly data collection and additional training. Inspired by the finding that Feed-Forward Network (FFN) in language models can act as \"key-value memory\", we propose Uncertainty-aware Observation Reinjection (UAOR), an effective, training-free and plug-and-play module for VLA models. Specifically, when the current language model layer exhibits high uncertainty, measured by Action Entropy, it reinjects key observation information into the next layer's Feed-Forward Network (FFN) through attention retrieval. This mechanism helps VLAs better attend to observations during inference, enabling more confident and faithful action generation. Comprehensive experiments show that our method consistently improves diverse VLA models across simulation and real-world tasks with minimal overhead. Notably, UAOR eliminates the need for additional observation cues or modules, making it a versatile and practical plug-in for existing VLA pipelines. The project page is at https://uaor.jiabingyang.cn.", "AI": {"tldr": "\u63d0\u51faUAOR\uff08\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u89c2\u6d4b\u91cd\u6ce8\u5165\uff09\u65b9\u6cd5\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684VLA\u6a21\u578b\u589e\u5f3a\u6a21\u5757\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u6d4b\u91cf\u548c\u6ce8\u610f\u529b\u68c0\u7d22\u673a\u5236\uff0c\u5728\u63a8\u7406\u65f6\u66f4\u597d\u5730\u5173\u6ce8\u89c2\u6d4b\u4fe1\u606f\uff0c\u63d0\u5347\u52a8\u4f5c\u751f\u6210\u7684\u7f6e\u4fe1\u5ea6\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u589e\u5f3a\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u989d\u5916\u7684\u89c2\u6d4b\u7ebf\u7d22\uff08\u5982\u6df1\u5ea6\u56fe\u3001\u70b9\u4e91\uff09\u6216\u8f85\u52a9\u6a21\u5757\uff08\u5982\u76ee\u6807\u68c0\u6d4b\u5668\u3001\u7f16\u7801\u5668\uff09\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684\u6570\u636e\u6536\u96c6\u548c\u989d\u5916\u8bad\u7ec3\u3002\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u6a21\u5757\u6765\u63d0\u5347VLA\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faUAOR\u6a21\u5757\uff0c\u5f53\u8bed\u8a00\u6a21\u578b\u5c42\u8868\u73b0\u51fa\u9ad8\u4e0d\u786e\u5b9a\u6027\uff08\u901a\u8fc7\u52a8\u4f5c\u71b5\u6d4b\u91cf\uff09\u65f6\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u68c0\u7d22\u673a\u5236\u5c06\u5173\u952e\u89c2\u6d4b\u4fe1\u606f\u91cd\u65b0\u6ce8\u5165\u5230\u4e0b\u4e00\u5c42\u7684FFN\u4e2d\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u8bed\u8a00\u6a21\u578b\u4e2dFFN\u4f5c\u4e3a\"\u952e\u503c\u8bb0\u5fc6\"\u7684\u7279\u6027\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u589e\u5f3a\u5bf9\u89c2\u6d4b\u4fe1\u606f\u7684\u5173\u6ce8\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cdVLA\u6a21\u578b\u4e0a\u90fd\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd\uff0c\u8986\u76d6\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\uff0c\u4e14\u5f00\u9500\u6781\u5c0f\u3002\u7279\u522b\u5730\uff0cUAOR\u65e0\u9700\u989d\u5916\u89c2\u6d4b\u7ebf\u7d22\u6216\u6a21\u5757\uff0c\u6210\u4e3a\u73b0\u6709VLA\u7ba1\u9053\u7684\u901a\u7528\u5b9e\u7528\u63d2\u4ef6\u3002", "conclusion": "UAOR\u662f\u4e00\u79cd\u6709\u6548\u3001\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684VLA\u6a21\u578b\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u89c2\u6d4b\u91cd\u6ce8\u5165\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5177\u6709\u5f88\u597d\u7684\u901a\u7528\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2602.18022", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18022", "abs": "https://arxiv.org/abs/2602.18022", "authors": ["Guandong Li", "Mengxia Ye"], "title": "Dual-Channel Attention Guidance for Training-Free Image Editing Control in Diffusion Transformers", "comment": null, "summary": "Training-free control over editing intensity is a critical requirement for diffusion-based image editing models built on the Diffusion Transformer (DiT) architecture. Existing attention manipulation methods focus exclusively on the Key space to modulate attention routing, leaving the Value space -- which governs feature aggregation -- entirely unexploited. In this paper, we first reveal that both Key and Value projections in DiT's multi-modal attention layers exhibit a pronounced bias-delta structure, where token embeddings cluster tightly around a layer-specific bias vector. Building on this observation, we propose Dual-Channel Attention Guidance (DCAG), a training-free framework that simultaneously manipulates both the Key channel (controlling where to attend) and the Value channel (controlling what to aggregate). We provide a theoretical analysis showing that the Key channel operates through the nonlinear softmax function, acting as a coarse control knob, while the Value channel operates through linear weighted summation, serving as a fine-grained complement. Together, the two-dimensional parameter space $(\u03b4_k, \u03b4_v)$ enables more precise editing-fidelity trade-offs than any single-channel method. Extensive experiments on the PIE-Bench benchmark (700 images, 10 editing categories) demonstrate that DCAG consistently outperforms Key-only guidance across all fidelity metrics, with the most significant improvements observed in localized editing tasks such as object deletion (4.9% LPIPS reduction) and object addition (3.2% LPIPS reduction).", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDCAG\u6846\u67b6\uff0c\u901a\u8fc7\u540c\u65f6\u64cd\u7eb5DiT\u67b6\u6784\u4e2d\u7684Key\u548cValue\u901a\u9053\u5b9e\u73b0\u65e0\u8bad\u7ec3\u7f16\u8f91\u5f3a\u5ea6\u63a7\u5236\uff0c\u76f8\u6bd4\u4ec5\u64cd\u4f5cKey\u7684\u65b9\u6cd5\u5728\u7f16\u8f91\u4fdd\u771f\u5ea6\u6743\u8861\u4e0a\u66f4\u7cbe\u786e\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eDiT\u67b6\u6784\u7684\u6269\u6563\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u9700\u8981\u65e0\u8bad\u7ec3\u7684\u7f16\u8f91\u5f3a\u5ea6\u63a7\u5236\u3002\u73b0\u6709\u6ce8\u610f\u529b\u64cd\u7eb5\u65b9\u6cd5\u4ec5\u5173\u6ce8Key\u7a7a\u95f4\u6765\u8c03\u8282\u6ce8\u610f\u529b\u8def\u7531\uff0c\u800c\u5b8c\u5168\u5ffd\u7565\u4e86\u63a7\u5236\u7279\u5f81\u805a\u5408\u7684Value\u7a7a\u95f4\u3002", "method": "\u9996\u5148\u53d1\u73b0DiT\u591a\u6a21\u6001\u6ce8\u610f\u529b\u5c42\u4e2d\u7684Key\u548cValue\u6295\u5f71\u90fd\u8868\u73b0\u51fa\u660e\u663e\u7684\u504f\u7f6e-\u589e\u91cf\u7ed3\u6784\u3002\u57fa\u4e8e\u6b64\u63d0\u51fa\u53cc\u901a\u9053\u6ce8\u610f\u529b\u5f15\u5bfc(DCAG)\u6846\u67b6\uff0c\u540c\u65f6\u64cd\u7eb5Key\u901a\u9053(\u63a7\u5236\u5173\u6ce8\u4f4d\u7f6e)\u548cValue\u901a\u9053(\u63a7\u5236\u805a\u5408\u5185\u5bb9)\u3002\u7406\u8bba\u5206\u6790\u8868\u660eKey\u901a\u9053\u901a\u8fc7\u975e\u7ebf\u6027softmax\u51fd\u6570\u4f5c\u4e3a\u7c97\u7c92\u5ea6\u63a7\u5236\uff0cValue\u901a\u9053\u901a\u8fc7\u7ebf\u6027\u52a0\u6743\u6c42\u548c\u4f5c\u4e3a\u7ec6\u7c92\u5ea6\u8865\u5145\u3002", "result": "\u5728PIE-Bench\u57fa\u51c6\u6d4b\u8bd5(700\u5f20\u56fe\u50cf\uff0c10\u4e2a\u7f16\u8f91\u7c7b\u522b)\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDCAG\u5728\u6240\u6709\u4fdd\u771f\u5ea6\u6307\u6807\u4e0a\u90fd\u6301\u7eed\u4f18\u4e8e\u4ec5Key\u5f15\u5bfc\u65b9\u6cd5\uff0c\u5728\u5bf9\u8c61\u5220\u9664(4.9% LPIPS\u51cf\u5c11)\u548c\u5bf9\u8c61\u6dfb\u52a0(3.2% LPIPS\u51cf\u5c11)\u7b49\u5c40\u90e8\u7f16\u8f91\u4efb\u52a1\u4e2d\u6539\u8fdb\u6700\u663e\u8457\u3002", "conclusion": "\u4e8c\u7ef4\u53c2\u6570\u7a7a\u95f4(\u03b4_k, \u03b4_v)\u80fd\u591f\u5b9e\u73b0\u6bd4\u4efb\u4f55\u5355\u901a\u9053\u65b9\u6cd5\u66f4\u7cbe\u786e\u7684\u7f16\u8f91-\u4fdd\u771f\u5ea6\u6743\u8861\uff0cDCAG\u6846\u67b6\u4e3a\u57fa\u4e8eDiT\u7684\u6269\u6563\u7f16\u8f91\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u7cbe\u7ec6\u7684\u63a7\u5236\u80fd\u529b\u3002"}}
{"id": "2602.18043", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18043", "abs": "https://arxiv.org/abs/2602.18043", "authors": ["Hongyu Qu", "Xiangbo Shu", "Rui Yan", "Hailiang Gao", "Wenguan Wang", "Jinhui Tang"], "title": "Spatio-temporal Decoupled Knowledge Compensator for Few-Shot Action Recognition", "comment": "Accepted to TPAMI 2026", "summary": "Few-Shot Action Recognition (FSAR) is a challenging task that requires recognizing novel action categories with a few labeled videos. Recent works typically apply semantically coarse category names as auxiliary contexts to guide the learning of discriminative visual features. However, such context provided by the action names is too limited to provide sufficient background knowledge for capturing novel spatial and temporal concepts in actions. In this paper, we propose DiST, an innovative Decomposition-incorporation framework for FSAR that makes use of decoupled Spatial and Temporal knowledge provided by large language models to learn expressive multi-granularity prototypes. In the decomposition stage, we decouple vanilla action names into diverse spatio-temporal attribute descriptions (action-related knowledge). Such commonsense knowledge complements semantic contexts from spatial and temporal perspectives. In the incorporation stage, we propose Spatial/Temporal Knowledge Compensators (SKC/TKC) to discover discriminative object-level and frame-level prototypes, respectively. In SKC, object-level prototypes adaptively aggregate important patch tokens under the guidance of spatial knowledge. Moreover, in TKC, frame-level prototypes utilize temporal attributes to assist in inter-frame temporal relation modeling. These learned prototypes thus provide transparency in capturing fine-grained spatial details and diverse temporal patterns. Experimental results show DiST achieves state-of-the-art results on five standard FSAR datasets.", "AI": {"tldr": "DiST\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5c11\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u7684\u5206\u89e3-\u6574\u5408\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u7684\u89e3\u8026\u7a7a\u95f4\u548c\u65f6\u95f4\u77e5\u8bc6\u6765\u5b66\u4e60\u8868\u8fbe\u6027\u591a\u7c92\u5ea6\u539f\u578b\uff0c\u5728\u4e94\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5c11\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u8bed\u4e49\u7c97\u7cd9\u7684\u7c7b\u522b\u540d\u79f0\u4f5c\u4e3a\u8f85\u52a9\u4e0a\u4e0b\u6587\uff0c\u4f46\u8fd9\u79cd\u4e0a\u4e0b\u6587\u8fc7\u4e8e\u6709\u9650\uff0c\u65e0\u6cd5\u4e3a\u6355\u6349\u52a8\u4f5c\u4e2d\u7684\u65b0\u9896\u7a7a\u95f4\u548c\u65f6\u95f4\u6982\u5ff5\u63d0\u4f9b\u8db3\u591f\u7684\u80cc\u666f\u77e5\u8bc6\u3002", "method": "\u63d0\u51faDiST\u6846\u67b6\uff1a1)\u5206\u89e3\u9636\u6bb5\uff1a\u5c06\u539f\u59cb\u52a8\u4f5c\u540d\u79f0\u89e3\u8026\u4e3a\u591a\u6837\u5316\u7684\u65f6\u7a7a\u5c5e\u6027\u63cf\u8ff0\uff1b2)\u6574\u5408\u9636\u6bb5\uff1a\u63d0\u51fa\u7a7a\u95f4/\u65f6\u95f4\u77e5\u8bc6\u8865\u507f\u5668(SKC/TKC)\u5206\u522b\u53d1\u73b0\u5224\u522b\u6027\u5bf9\u8c61\u7ea7\u548c\u5e27\u7ea7\u539f\u578b\uff0cSKC\u5728\u7a7a\u95f4\u77e5\u8bc6\u6307\u5bfc\u4e0b\u81ea\u9002\u5e94\u805a\u5408\u91cd\u8981\u8865\u4e01\u6807\u8bb0\uff0cTKC\u5229\u7528\u65f6\u95f4\u5c5e\u6027\u8f85\u52a9\u5e27\u95f4\u65f6\u95f4\u5173\u7cfb\u5efa\u6a21\u3002", "result": "\u5728\u4e94\u4e2a\u6807\u51c6\u5c11\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "DiST\u901a\u8fc7\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u7684\u89e3\u8026\u7a7a\u95f4\u548c\u65f6\u95f4\u77e5\u8bc6\uff0c\u80fd\u591f\u5b66\u4e60\u8868\u8fbe\u6027\u591a\u7c92\u5ea6\u539f\u578b\uff0c\u4e3a\u6355\u6349\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u7ec6\u8282\u548c\u591a\u6837\u5316\u65f6\u95f4\u6a21\u5f0f\u63d0\u4f9b\u4e86\u900f\u660e\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c11\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\u3002"}}
{"id": "2602.18047", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18047", "abs": "https://arxiv.org/abs/2602.18047", "authors": ["Rong Fu", "Wenxin Zhang", "Yibo Meng", "Jia Yee Tan", "Jiaxuan Lu", "Rui Lu", "Jiekai Wu", "Zhaolu Kang", "Simon Fong"], "title": "CityGuard: Graph-Aware Private Descriptors for Bias-Resilient Identity Search Across Urban Cameras", "comment": "36 pages, 12 figures", "summary": "City-scale person re-identification across distributed cameras must handle severe appearance changes from viewpoint, occlusion, and domain shift while complying with data protection rules that prevent sharing raw imagery. We introduce CityGuard, a topology-aware transformer for privacy-preserving identity retrieval in decentralized surveillance. The framework integrates three components. A dispersion-adaptive metric learner adjusts instance-level margins according to feature spread, increasing intra-class compactness. Spatially conditioned attention injects coarse geometry, such as GPS or deployment floor plans, into graph-based self-attention to enable projectively consistent cross-view alignment using only coarse geometric priors without requiring survey-grade calibration. Differentially private embedding maps are coupled with compact approximate indexes to support secure and cost-efficient deployment. Together these designs produce descriptors robust to viewpoint variation, occlusion, and domain shifts, and they enable a tunable balance between privacy and utility under rigorous differential-privacy accounting. Experiments on Market-1501 and additional public benchmarks, complemented by database-scale retrieval studies, show consistent gains in retrieval precision and query throughput over strong baselines, confirming the practicality of the framework for privacy-critical urban identity matching.", "AI": {"tldr": "CityGuard\uff1a\u4e00\u79cd\u7528\u4e8e\u5206\u5e03\u5f0f\u76d1\u63a7\u4e2d\u9690\u79c1\u4fdd\u62a4\u8eab\u4efd\u68c0\u7d22\u7684\u62d3\u6251\u611f\u77e5Transformer\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6563\u81ea\u9002\u5e94\u5ea6\u91cf\u5b66\u4e60\u3001\u7a7a\u95f4\u6761\u4ef6\u6ce8\u610f\u529b\u548c\u5dee\u5206\u9690\u79c1\u5d4c\u5165\u6620\u5c04\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u8de8\u89c6\u89d2\u8eab\u4efd\u5339\u914d\u3002", "motivation": "\u57ce\u5e02\u89c4\u6a21\u7684\u4eba\u5458\u518d\u8bc6\u522b\u9762\u4e34\u89c6\u89d2\u53d8\u5316\u3001\u906e\u6321\u548c\u57df\u504f\u79fb\u7b49\u6311\u6218\uff0c\u540c\u65f6\u9700\u8981\u9075\u5b88\u6570\u636e\u4fdd\u62a4\u6cd5\u89c4\uff0c\u9632\u6b62\u539f\u59cb\u56fe\u50cf\u5171\u4eab\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u8de8\u89c6\u89d2\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "1. \u5206\u6563\u81ea\u9002\u5e94\u5ea6\u91cf\u5b66\u4e60\uff1a\u6839\u636e\u7279\u5f81\u5206\u5e03\u8c03\u6574\u5b9e\u4f8b\u7ea7\u8fb9\u754c\uff0c\u589e\u5f3a\u7c7b\u5185\u7d27\u51d1\u6027\uff1b2. \u7a7a\u95f4\u6761\u4ef6\u6ce8\u610f\u529b\uff1a\u5c06GPS\u6216\u90e8\u7f72\u5e73\u9762\u56fe\u7b49\u7c97\u7565\u51e0\u4f55\u4fe1\u606f\u6ce8\u5165\u57fa\u4e8e\u56fe\u7684\u81ea\u6ce8\u610f\u529b\uff0c\u5b9e\u73b0\u6295\u5f71\u4e00\u81f4\u7684\u8de8\u89c6\u89d2\u5bf9\u9f50\uff1b3. \u5dee\u5206\u9690\u79c1\u5d4c\u5165\u6620\u5c04\u4e0e\u7d27\u51d1\u8fd1\u4f3c\u7d22\u5f15\uff1a\u652f\u6301\u5b89\u5168\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u90e8\u7f72\u3002", "result": "\u5728Market-1501\u548c\u5176\u4ed6\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u68c0\u7d22\u7cbe\u5ea6\u548c\u67e5\u8be2\u541e\u5410\u91cf\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6570\u636e\u5e93\u89c4\u6a21\u68c0\u7d22\u7814\u7a76\u8bc1\u5b9e\u4e86\u6846\u67b6\u5728\u9690\u79c1\u5173\u952e\u57ce\u5e02\u8eab\u4efd\u5339\u914d\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "CityGuard\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u89c6\u89d2\u53d8\u5316\u3001\u906e\u6321\u548c\u57df\u504f\u79fb\uff0c\u5728\u4e25\u683c\u7684\u5dee\u5206\u9690\u79c1\u6838\u7b97\u4e0b\u5b9e\u73b0\u9690\u79c1\u4e0e\u6548\u7528\u7684\u53ef\u8c03\u5e73\u8861\uff0c\u4e3a\u9690\u79c1\u5173\u952e\u7684\u57ce\u5e02\u8eab\u4efd\u5339\u914d\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18057", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18057", "abs": "https://arxiv.org/abs/2602.18057", "authors": ["Hongsong Wang", "Wenjing Yan", "Qiuxia Lai", "Xin Geng"], "title": "Temporal Consistency-Aware Text-to-Motion Generation", "comment": "Code is on https://github.com/Giat995/TCA-T2M/", "summary": "Text-to-Motion (T2M) generation aims to synthesize realistic human motion sequences from natural language descriptions. While two-stage frameworks leveraging discrete motion representations have advanced T2M research, they often neglect cross-sequence temporal consistency, i.e., the shared temporal structures present across different instances of the same action. This leads to semantic misalignments and physically implausible motions. To address this limitation, we propose TCA-T2M, a framework for temporal consistency-aware T2M generation. Our approach introduces a temporal consistency-aware spatial VQ-VAE (TCaS-VQ-VAE) for cross-sequence temporal alignment, coupled with a masked motion transformer for text-conditioned motion generation. Additionally, a kinematic constraint block mitigates discretization artifacts to ensure physical plausibility. Experiments on HumanML3D and KIT-ML benchmarks demonstrate that TCA-T2M achieves state-of-the-art performance, highlighting the importance of temporal consistency in robust and coherent T2M generation.", "AI": {"tldr": "TCA-T2M\u662f\u4e00\u4e2a\u65f6\u95f4\u4e00\u81f4\u6027\u611f\u77e5\u7684\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u5e8f\u5217\u65f6\u95f4\u5bf9\u9f50\u548c\u8fd0\u52a8\u7ea6\u675f\u63d0\u5347\u52a8\u4f5c\u751f\u6210\u7684\u8bed\u4e49\u5bf9\u9f50\u548c\u7269\u7406\u5408\u7406\u6027\u3002", "motivation": "\u73b0\u6709\u4e24\u9636\u6bb5\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u6846\u67b6\u901a\u5e38\u5ffd\u89c6\u8de8\u5e8f\u5217\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u8bed\u4e49\u9519\u4f4d\u548c\u7269\u7406\u4e0a\u4e0d\u53ef\u4fe1\u7684\u52a8\u4f5c\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u6765\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "method": "\u63d0\u51faTCA-T2M\u6846\u67b6\uff1a1) \u65f6\u95f4\u4e00\u81f4\u6027\u611f\u77e5\u7a7a\u95f4VQ-VAE\u7528\u4e8e\u8de8\u5e8f\u5217\u65f6\u95f4\u5bf9\u9f50\uff1b2) \u63a9\u7801\u8fd0\u52a8\u53d8\u6362\u5668\u7528\u4e8e\u6587\u672c\u6761\u4ef6\u52a8\u4f5c\u751f\u6210\uff1b3) \u8fd0\u52a8\u5b66\u7ea6\u675f\u5757\u51cf\u5c11\u79bb\u6563\u5316\u4f2a\u5f71\u786e\u4fdd\u7269\u7406\u5408\u7406\u6027\u3002", "result": "\u5728HumanML3D\u548cKIT-ML\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u65f6\u95f4\u4e00\u81f4\u6027\u5bf9\u9c81\u68d2\u548c\u8fde\u8d2f\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u65f6\u95f4\u4e00\u81f4\u6027\u662f\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u7684\u5173\u952e\u56e0\u7d20\uff0cTCA-T2M\u901a\u8fc7\u8de8\u5e8f\u5217\u65f6\u95f4\u5bf9\u9f50\u548c\u7269\u7406\u7ea6\u675f\u6709\u6548\u63d0\u5347\u4e86\u52a8\u4f5c\u751f\u6210\u7684\u8bed\u4e49\u51c6\u786e\u6027\u548c\u7269\u7406\u5408\u7406\u6027\u3002"}}
{"id": "2602.18066", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18066", "abs": "https://arxiv.org/abs/2602.18066", "authors": ["Daniel Busch", "Christian Bohn", "Thomas Kurbiel", "Klaus Friedrichs", "Richard Meyes", "Tobias Meisen"], "title": "Faster Training, Fewer Labels: Self-Supervised Pretraining for Fine-Grained BEV Segmentation", "comment": "This Paper has been accepted to the 2026 IEEE Intelligent Vehicles Symposium (IV)", "summary": "Dense Bird's Eye View (BEV) semantic maps are central to autonomous driving, yet current multi-camera methods depend on costly, inconsistently annotated BEV ground truth. We address this limitation with a two-phase training strategy for fine-grained road marking segmentation that removes full supervision during pretraining and halves the amount of training data during fine-tuning while still outperforming the comparable supervised baseline model. During the self-supervised pretraining, BEVFormer predictions are differentiably reprojected into the image plane and trained against multi-view semantic pseudo-labels generated by the widely used semantic segmentation model Mask2Former. A temporal loss encourages consistency across frames. The subsequent supervised fine-tuning phase requires only 50% of the dataset and significantly less training time. With our method, the fine-tuning benefits from rich priors learned during pretraining boosting the performance and BEV segmentation quality (up to +2.5pp mIoU over the fully supervised baseline) on nuScenes. It simultaneously halves the usage of annotation data and reduces total training time by up to two thirds. The results demonstrate that differentiable reprojection plus camera perspective pseudo labels yields transferable BEV features and a scalable path toward reduced-label autonomous perception.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u534a\u76d1\u7763\u5fae\u8c03\uff0c\u5728\u51cf\u5c11\u6807\u6ce8\u6570\u636e\u4f7f\u7528\u7684\u540c\u65f6\u63d0\u5347BEV\u8bed\u4e49\u5730\u56fe\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6444\u50cf\u5934BEV\u8bed\u4e49\u5730\u56fe\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u4e14\u6807\u6ce8\u4e0d\u4e00\u81f4\u7684\u5730\u9762\u771f\u503c\u6570\u636e\uff0c\u8fd9\u9650\u5236\u4e86\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1\uff09\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u5c06BEVFormer\u9884\u6d4b\u53ef\u5fae\u5206\u5730\u91cd\u6295\u5f71\u5230\u56fe\u50cf\u5e73\u9762\uff0c\u4f7f\u7528Mask2Former\u751f\u6210\u7684\u591a\u89c6\u89d2\u8bed\u4e49\u4f2a\u6807\u7b7e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u52a0\u5165\u65f6\u95f4\u4e00\u81f4\u6027\u635f\u5931\uff1b2\uff09\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\uff0c\u4ec5\u970050%\u6807\u6ce8\u6570\u636e\u548c\u66f4\u5c11\u8bad\u7ec3\u65f6\u95f4\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u5168\u76d1\u7763\u57fa\u7ebf\u63d0\u5347\u9ad8\u8fbe2.5pp mIoU\uff0c\u540c\u65f6\u51cf\u5c1150%\u6807\u6ce8\u6570\u636e\u4f7f\u7528\uff0c\u603b\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u4e09\u5206\u4e4b\u4e8c\u3002", "conclusion": "\u53ef\u5fae\u5206\u91cd\u6295\u5f71\u52a0\u76f8\u673a\u89c6\u89d2\u4f2a\u6807\u7b7e\u80fd\u591f\u4ea7\u751f\u53ef\u8fc1\u79fb\u7684BEV\u7279\u5f81\uff0c\u4e3a\u51cf\u5c11\u6807\u6ce8\u7684\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002"}}
{"id": "2602.18089", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18089", "abs": "https://arxiv.org/abs/2602.18089", "authors": ["Kunwar Arpit Singh", "Ankush Prakash", "Haroon R Lone"], "title": "DohaScript: A Large-Scale Multi-Writer Dataset for Continuous Handwritten Hindi Text", "comment": null, "summary": "Despite having hundreds of millions of speakers, handwritten Devanagari text remains severely underrepresented in publicly available benchmark datasets. Existing resources are limited in scale, focus primarily on isolated characters or short words, and lack controlled lexical content and writer level diversity, which restricts their utility for modern data driven handwriting analysis. As a result, they fail to capture the continuous, fused, and structurally complex nature of Devanagari handwriting, where characters are connected through a shared shirorekha (horizontal headline) and exhibit rich ligature formations. We introduce DohaScript, a large scale, multi writer dataset of handwritten Hindi text collected from 531 unique contributors. The dataset is designed as a parallel stylistic corpus, in which all writers transcribe the same fixed set of six traditional Hindi dohas (couplets). This controlled design enables systematic analysis of writer specific variation independent of linguistic content, and supports tasks such as handwriting recognition, writer identification, style analysis, and generative modeling. The dataset is accompanied by non identifiable demographic metadata, rigorous quality curation based on objective sharpness and resolution criteria, and page level layout difficulty annotations that facilitate stratified benchmarking. Baseline experiments demonstrate clear quality separation and strong generalization to unseen writers, highlighting the dataset's reliability and practical value. DohaScript is intended to serve as a standardized and reproducible benchmark for advancing research on continuous handwritten Devanagari text in low resource script settings.", "AI": {"tldr": "DohaScript\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u4e66\u5199\u8005\u7684\u624b\u5199\u5370\u5730\u8bed\u6570\u636e\u96c6\uff0c\u5305\u542b531\u540d\u8d21\u732e\u8005\u4e66\u5199\u7684\u76f8\u540c\u516d\u9996\u4f20\u7edf\u5370\u5730\u8bed\u5bf9\u53e5\uff0c\u7528\u4e8e\u652f\u6301\u624b\u5199\u8bc6\u522b\u3001\u4e66\u5199\u8005\u8bc6\u522b\u548c\u98ce\u683c\u5206\u6790\u7b49\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u624b\u5199\u5929\u57ce\u4f53\u6587\u672c\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\uff0c\u4e3b\u8981\u5173\u6ce8\u5b64\u7acb\u5b57\u7b26\u6216\u77ed\u8bcd\uff0c\u7f3a\u4e4f\u53d7\u63a7\u8bcd\u6c47\u5185\u5bb9\u548c\u4e66\u5199\u8005\u591a\u6837\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u5929\u57ce\u4f53\u624b\u5199\u4e2d\u5b57\u7b26\u901a\u8fc7\u5171\u4eab\u6a2a\u7ebf\u8fde\u63a5\u548c\u4e30\u5bcc\u8fde\u5b57\u5f62\u6210\u7684\u8fde\u7eed\u3001\u878d\u5408\u548c\u7ed3\u6784\u590d\u6742\u7279\u6027\u3002", "method": "\u6536\u96c6531\u540d\u72ec\u7279\u8d21\u732e\u8005\u7684\u624b\u5199\u5370\u5730\u8bed\u6587\u672c\uff0c\u8bbe\u8ba1\u4e3a\u5e73\u884c\u98ce\u683c\u8bed\u6599\u5e93\uff0c\u6240\u6709\u4e66\u5199\u8005\u8f6c\u5f55\u76f8\u540c\u7684\u516d\u9996\u4f20\u7edf\u5370\u5730\u8bed\u5bf9\u53e5\u3002\u6570\u636e\u96c6\u5305\u542b\u975e\u8bc6\u522b\u6027\u4eba\u53e3\u7edf\u8ba1\u5143\u6570\u636e\uff0c\u57fa\u4e8e\u5ba2\u89c2\u6e05\u6670\u5ea6\u548c\u5206\u8fa8\u7387\u6807\u51c6\u7684\u4e25\u683c\u8d28\u91cf\u7b5b\u9009\uff0c\u4ee5\u53ca\u9875\u9762\u7ea7\u5e03\u5c40\u96be\u5ea6\u6807\u6ce8\u3002", "result": "\u57fa\u7ebf\u5b9e\u9a8c\u663e\u793a\u51fa\u6e05\u6670\u7684\u8d28\u91cf\u5206\u79bb\u548c\u5bf9\u672a\u89c1\u4e66\u5199\u8005\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u7a81\u51fa\u4e86\u6570\u636e\u96c6\u7684\u53ef\u9760\u6027\u548c\u5b9e\u7528\u4ef7\u503c\u3002DohaScript\u53ef\u4f5c\u4e3a\u6807\u51c6\u5316\u3001\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "conclusion": "DohaScript\u65e8\u5728\u4f5c\u4e3a\u6807\u51c6\u5316\u3001\u53ef\u590d\u73b0\u7684\u57fa\u51c6\uff0c\u63a8\u52a8\u4f4e\u8d44\u6e90\u811a\u672c\u73af\u5883\u4e0b\u8fde\u7eed\u624b\u5199\u5929\u57ce\u4f53\u6587\u672c\u7684\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2602.18093", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18093", "abs": "https://arxiv.org/abs/2602.18093", "authors": ["Hanshuai Cui", "Zhiqing Tang", "Qianli Ma", "Zhi Yao", "Weijia Jia"], "title": "Predict to Skip: Linear Multistep Feature Forecasting for Efficient Diffusion Transformers", "comment": null, "summary": "Diffusion Transformers (DiT) have emerged as a widely adopted backbone for high-fidelity image and video generation, yet their iterative denoising process incurs high computational costs. Existing training-free acceleration methods rely on feature caching and reuse under the assumption of temporal stability. However, reusing features for multiple steps may lead to latent drift and visual degradation. We observe that model outputs evolve smoothly along much of the diffusion trajectory, enabling principled predictions rather than naive reuse. Based on this insight, we propose \\textbf{PrediT}, a training-free acceleration framework that formulates feature prediction as a linear multistep problem. We employ classical linear multistep methods to forecast future model outputs from historical information, combined with a corrector that activates in high-dynamics regions to prevent error accumulation. A dynamic step modulation mechanism adaptively adjusts the prediction horizon by monitoring the feature change rate. Together, these components enable substantial acceleration while preserving generation fidelity. Extensive experiments validate that our method achieves up to $5.54\\times$ latency reduction across various DiT-based image and video generation models, while incurring negligible quality degradation.", "AI": {"tldr": "PrediT\uff1a\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684DiT\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u7ebf\u6027\u591a\u6b65\u65b9\u6cd5\u9884\u6d4b\u6a21\u578b\u8f93\u51fa\uff0c\u7ed3\u5408\u6821\u6b63\u5668\u548c\u52a8\u6001\u6b65\u957f\u8c03\u5236\uff0c\u5b9e\u73b05.54\u500d\u5ef6\u8fdf\u964d\u4f4e\u4e14\u8d28\u91cf\u635f\u5931\u53ef\u5ffd\u7565\u3002", "motivation": "DiT\u5728\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u73b0\u6709\u514d\u8bad\u7ec3\u52a0\u901f\u65b9\u6cd5\u4f9d\u8d56\u7279\u5f81\u7f13\u5b58\u548c\u91cd\u7528\uff0c\u4f46\u591a\u6b65\u91cd\u7528\u4f1a\u5bfc\u81f4\u6f5c\u5728\u6f02\u79fb\u548c\u89c6\u89c9\u8d28\u91cf\u4e0b\u964d\u3002\u4f5c\u8005\u89c2\u5bdf\u5230\u6a21\u578b\u8f93\u51fa\u5728\u6269\u6563\u8f68\u8ff9\u7684\u5927\u90e8\u5206\u533a\u57df\u5e73\u6ed1\u6f14\u5316\uff0c\u8fd9\u4e3a\u9884\u6d4b\u800c\u975e\u7b80\u5355\u91cd\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "method": "\u63d0\u51faPrediT\u6846\u67b6\uff1a1) \u5c06\u7279\u5f81\u9884\u6d4b\u5efa\u6a21\u4e3a\u7ebf\u6027\u591a\u6b65\u95ee\u9898\uff0c\u4f7f\u7528\u7ecf\u5178\u7ebf\u6027\u591a\u6b65\u65b9\u6cd5\u4ece\u5386\u53f2\u4fe1\u606f\u9884\u6d4b\u672a\u6765\u6a21\u578b\u8f93\u51fa\uff1b2) \u5728\u9ad8\u52a8\u6001\u533a\u57df\u6fc0\u6d3b\u6821\u6b63\u5668\u9632\u6b62\u8bef\u5dee\u7d2f\u79ef\uff1b3) \u901a\u8fc7\u76d1\u6d4b\u7279\u5f81\u53d8\u5316\u7387\u81ea\u9002\u5e94\u8c03\u6574\u9884\u6d4b\u6b65\u957f\u7684\u52a8\u6001\u6b65\u957f\u8c03\u5236\u673a\u5236\u3002", "result": "\u5728\u591a\u79cd\u57fa\u4e8eDiT\u7684\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e0a\uff0cPrediT\u5b9e\u73b0\u4e86\u9ad8\u8fbe5.54\u500d\u7684\u5ef6\u8fdf\u964d\u4f4e\uff0c\u540c\u65f6\u8d28\u91cf\u9000\u5316\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "PrediT\u901a\u8fc7\u9884\u6d4b\u800c\u975e\u7b80\u5355\u91cd\u7528\u7279\u5f81\uff0c\u7ed3\u5408\u6821\u6b63\u673a\u5236\u548c\u81ea\u9002\u5e94\u6b65\u957f\u63a7\u5236\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u52a0\u901fDiT\u63a8\u7406\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u5b9e\u7528\u5316\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18094", "categories": ["cs.CV", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.18094", "abs": "https://arxiv.org/abs/2602.18094", "authors": ["Ling Lin", "Yang Bai", "Heng Su", "Congcong Zhu", "Yaoxing Wang", "Yang Zhou", "Huazhu Fu", "Jingrun Chen"], "title": "OODBench: Out-of-Distribution Benchmark for Large Vision-Language Models", "comment": "54 pages, 21 figures", "summary": "Existing Visual-Language Models (VLMs) have achieved significant progress by being trained on massive-scale datasets, typically under the assumption that data are independent and identically distributed (IID). However, in real-world scenarios, it is often impractical to expect that all data processed by an AI system satisfy this assumption. Furthermore, failure to appropriately handle out-of-distribution (OOD) objects may introduce safety risks in real-world applications (e.g., autonomous driving or medical assistance). Unfortunately, current research has not yet provided valid benchmarks that can comprehensively assess the performance of VLMs in response to OOD data. Therefore, we propose OODBench, a predominantly automated method with minimal human verification, for constructing new benchmarks and evaluating the ability of VLMs to process OOD data. OODBench contains 40K instance-level OOD instance-category pairs, and we show that current VLMs still exhibit notable performance degradation on OODBench, even when the underlying image categories are common. In addition, we propose a reliable automated assessment metric that employs a Basic-to-Advanced Progression of prompted questions to assess the impact of OOD data on questions of varying difficulty more fully. Lastly, we summarize substantial findings and insights to facilitate future research in the acquisition and evaluation of OOD data.", "AI": {"tldr": "OODBench\uff1a\u9996\u4e2a\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5904\u7406\u5206\u5e03\u5916\u6570\u636e\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u5305\u542b40K\u5b9e\u4f8b\u7ea7OOD\u6570\u636e\u5bf9\uff0c\u63ed\u793a\u4e86\u73b0\u6709VLM\u5728OOD\u6570\u636e\u4e0a\u7684\u663e\u8457\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u5047\u8bbe\u6570\u636e\u72ec\u7acb\u540c\u5206\u5e03\uff0c\u4f46\u73b0\u5b9e\u5e94\u7528\u4e2d\u5e38\u9047\u5230\u5206\u5e03\u5916\u6570\u636e\uff0c\u5904\u7406\u4e0d\u5f53\u53ef\u80fd\u5e26\u6765\u5b89\u5168\u98ce\u9669\u3002\u76ee\u524d\u7f3a\u4e4f\u5168\u9762\u8bc4\u4f30VLM\u5904\u7406OOD\u6570\u636e\u80fd\u529b\u7684\u6709\u6548\u57fa\u51c6\u3002", "method": "\u63d0\u51faOODBench\uff0c\u4e00\u79cd\u4e3b\u8981\u81ea\u52a8\u5316\u3001\u6700\u5c0f\u4eba\u5de5\u9a8c\u8bc1\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6784\u5efa\u65b0\u57fa\u51c6\u5e76\u8bc4\u4f30VLM\u5904\u7406OOD\u6570\u636e\u7684\u80fd\u529b\u3002\u5305\u542b40K\u5b9e\u4f8b\u7ea7OOD\u5b9e\u4f8b-\u7c7b\u522b\u5bf9\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u57fa\u7840\u5230\u9ad8\u7ea7\u6e10\u8fdb\u5f0f\u63d0\u793a\u95ee\u9898\u7684\u53ef\u9760\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5f53\u524dVLM\u5728OODBench\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u4e0b\u964d\uff0c\u5373\u4f7f\u5e95\u5c42\u56fe\u50cf\u7c7b\u522b\u662f\u5e38\u89c1\u7684\u3002\u63d0\u51fa\u7684\u8bc4\u4f30\u6307\u6807\u80fd\u66f4\u5168\u9762\u5730\u8bc4\u4f30OOD\u6570\u636e\u5bf9\u4e0d\u540c\u96be\u5ea6\u95ee\u9898\u7684\u5f71\u54cd\u3002", "conclusion": "OODBench\u586b\u8865\u4e86VLM\u5728\u5206\u5e03\u5916\u6570\u636e\u8bc4\u4f30\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u603b\u7ed3\u4e86\u91cd\u8981\u53d1\u73b0\u548c\u89c1\u89e3\uff0c\u4e3a\u672a\u6765OOD\u6570\u636e\u83b7\u53d6\u548c\u8bc4\u4f30\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.18178", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18178", "abs": "https://arxiv.org/abs/2602.18178", "authors": ["Poonam Poonam", "Pere-Pau V\u00e1zquez", "Timo Ropinski"], "title": "Evaluating Graphical Perception Capabilities of Vision Transformers", "comment": null, "summary": "Vision Transformers, ViTs, have emerged as a powerful alternative to convolutional neural networks, CNNs, in a variety of image-based tasks. While CNNs have previously been evaluated for their ability to perform graphical perception tasks, which are essential for interpreting visualizations, the perceptual capabilities of ViTs remain largely unexplored. In this work, we investigate the performance of ViTs in elementary visual judgment tasks inspired by the foundational studies of Cleveland and McGill, which quantified the accuracy of human perception across different visual encodings. Inspired by their study, we benchmark ViTs against CNNs and human participants in a series of controlled graphical perception tasks. Our results reveal that, although ViTs demonstrate strong performance in general vision tasks, their alignment with human-like graphical perception in the visualization domain is limited. This study highlights key perceptual gaps and points to important considerations for the application of ViTs in visualization systems and graphical perceptual modeling.", "AI": {"tldr": "ViTs\u5728\u53ef\u89c6\u5316\u56fe\u5f62\u611f\u77e5\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u5982\u4eba\u7c7b\uff0c\u4e0eCNN\u76f8\u6bd4\u4e5f\u5b58\u5728\u5dee\u8ddd", "motivation": "\u867d\u7136ViTs\u5728\u591a\u79cd\u56fe\u50cf\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u53ef\u89c6\u5316\u56fe\u5f62\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u800c\u8fd9\u7c7b\u4efb\u52a1\u5bf9\u89c6\u89c9\u5316\u7cfb\u7edf\u81f3\u5173\u91cd\u8981", "method": "\u57fa\u4e8eCleveland\u548cMcGill\u7684\u7ecf\u5178\u7814\u7a76\uff0c\u8bbe\u8ba1\u4e86\u4e00\u7cfb\u5217\u53d7\u63a7\u7684\u56fe\u5f62\u611f\u77e5\u4efb\u52a1\uff0c\u5c06ViTs\u4e0eCNNs\u548c\u4eba\u7c7b\u53c2\u4e0e\u8005\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30", "result": "ViTs\u5728\u901a\u7528\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u5f3a\u52b2\uff0c\u4f46\u5728\u53ef\u89c6\u5316\u9886\u57df\u7684\u56fe\u5f62\u611f\u77e5\u4efb\u52a1\u4e2d\uff0c\u5176\u4e0e\u4eba\u7c7b\u611f\u77e5\u7684\u4e00\u81f4\u6027\u6709\u9650\uff0c\u5b58\u5728\u660e\u663e\u7684\u611f\u77e5\u5dee\u8ddd", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86ViTs\u5728\u53ef\u89c6\u5316\u7cfb\u7edf\u4e2d\u7684\u611f\u77e5\u5c40\u9650\u6027\uff0c\u4e3aViTs\u5728\u89c6\u89c9\u5316\u7cfb\u7edf\u548c\u56fe\u5f62\u611f\u77e5\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003"}}
{"id": "2602.18193", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18193", "abs": "https://arxiv.org/abs/2602.18193", "authors": ["Yiran Yang", "Zhaowei Liu", "Yuan Yuan", "Yukun Song", "Xiong Ma", "Yinghao Song", "Xiangji Zeng", "Lu Sun", "Yulu Wang", "Hai Zhou", "Shuai Cui", "Zhaohan Gong", "Jiefei Zhang"], "title": "BLM-Guard: Explainable Multimodal Ad Moderation with Chain-of-Thought and Policy-Aligned Rewards", "comment": "7 pages, 3 figures. To appear in AAAI 2026", "summary": "Short-video platforms now host vast multimodal ads whose deceptive visuals, speech and subtitles demand finer-grained, policy-driven moderation than community safety filters. We present BLM-Guard, a content-audit framework for commercial ads that fuses Chain-of-Thought reasoning with rule-based policy principles and a critic-guided reward. A rule-driven ICoT data-synthesis pipeline jump-starts training by generating structured scene descriptions, reasoning chains and labels, cutting annotation costs. Reinforcement learning then refines the model using a composite reward balancing causal coherence with policy adherence. A multitask architecture models intra-modal manipulations (e.g., exaggerated imagery) and cross-modal mismatches (e.g., subtitle-speech drift), boosting robustness. Experiments on real short-video ads show BLM-Guard surpasses strong baselines in accuracy, consistency and generalization.", "AI": {"tldr": "BLM-Guard\u662f\u4e00\u4e2a\u7528\u4e8e\u77ed\u89c6\u9891\u5e7f\u544a\u5185\u5bb9\u5ba1\u6838\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u601d\u7ef4\u94fe\u63a8\u7406\u3001\u89c4\u5219\u7b56\u7565\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u67b6\u6784\u68c0\u6d4b\u6a21\u6001\u5185\u548c\u8de8\u6a21\u6001\u7684\u6b3a\u9a97\u6027\u5185\u5bb9\u3002", "motivation": "\u77ed\u89c6\u9891\u5e73\u53f0\u4e0a\u7684\u591a\u6a21\u6001\u5e7f\u544a\u5305\u542b\u6b3a\u9a97\u6027\u7684\u89c6\u89c9\u3001\u8bed\u97f3\u548c\u5b57\u5e55\u5185\u5bb9\uff0c\u9700\u8981\u6bd4\u793e\u533a\u5b89\u5168\u8fc7\u6ee4\u5668\u66f4\u7ec6\u7c92\u5ea6\u3001\u57fa\u4e8e\u7b56\u7565\u7684\u5ba1\u6838\u65b9\u6cd5\u3002", "method": "1. \u7ed3\u5408\u601d\u7ef4\u94fe\u63a8\u7406\u4e0e\u57fa\u4e8e\u89c4\u5219\u7684\u7b56\u7565\u539f\u5219\u548c\u6279\u8bc4\u5f15\u5bfc\u7684\u5956\u52b1\u673a\u5236\uff1b2. \u89c4\u5219\u9a71\u52a8\u7684ICoT\u6570\u636e\u5408\u6210\u7ba1\u9053\u751f\u6210\u7ed3\u6784\u5316\u573a\u666f\u63cf\u8ff0\u3001\u63a8\u7406\u94fe\u548c\u6807\u7b7e\uff1b3. \u5f3a\u5316\u5b66\u4e60\u4f7f\u7528\u590d\u5408\u5956\u52b1\u5e73\u8861\u56e0\u679c\u4e00\u81f4\u6027\u4e0e\u7b56\u7565\u9075\u5b88\uff1b4. \u591a\u4efb\u52a1\u67b6\u6784\u5efa\u6a21\u6a21\u6001\u5185\u64cd\u4f5c\u548c\u8de8\u6a21\u6001\u4e0d\u5339\u914d\u3002", "result": "\u5728\u771f\u5b9e\u77ed\u89c6\u9891\u5e7f\u544a\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBLM-Guard\u5728\u51c6\u786e\u6027\u3001\u4e00\u81f4\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u8d85\u8d8a\u4e86\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "BLM-Guard\u6846\u67b6\u4e3a\u5546\u4e1a\u5e7f\u544a\u5185\u5bb9\u5ba1\u6838\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u591a\u6a21\u6001\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u89c4\u5219\u9a71\u52a8\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6b3a\u9a97\u6027\u5185\u5bb9\u7684\u7cbe\u51c6\u68c0\u6d4b\u3002"}}
{"id": "2602.18199", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18199", "abs": "https://arxiv.org/abs/2602.18199", "authors": ["Gahyeon Shim", "Soogeun Park", "Hyemin Ahn"], "title": "A Self-Supervised Approach on Motion Calibration for Enhancing Physical Plausibility in Text-to-Motion", "comment": null, "summary": "Generating semantically aligned human motion from textual descriptions has made rapid progress, but ensuring both semantic and physical realism in motion remains a challenge. In this paper, we introduce the Distortion-aware Motion Calibrator (DMC), a post-hoc module that refines physically implausible motions (e.g., foot floating) while preserving semantic consistency with the original textual description. Rather than relying on complex physical modeling, we propose a self-supervised and data-driven approach, whereby DMC learns to obtain physically plausible motions when an intentionally distorted motion and the original textual descriptions are given as inputs. We evaluate DMC as a post-hoc module to improve motions obtained from various text-to-motion generation models and demonstrate its effectiveness in improving physical plausibility while enhancing semantic consistency. The experimental results show that DMC reduces FID score by 42.74% on T2M and 13.20% on T2M-GPT, while also achieving the highest R-Precision. When applied to high-quality models like MoMask, DMC improves the physical plausibility of motions by reducing penetration by 33.0% as well as adjusting floating artifacts closer to the ground-truth reference. These results highlight that DMC can serve as a promising post-hoc motion refinement framework for any kind of text-to-motion models by incorporating textual semantics and physical plausibility.", "AI": {"tldr": "DMC\u662f\u4e00\u4e2a\u540e\u5904\u7406\u6a21\u5757\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u4fee\u6b63\u6587\u672c\u751f\u6210\u52a8\u4f5c\u4e2d\u7684\u7269\u7406\u4e0d\u5408\u7406\u6027\uff08\u5982\u811a\u90e8\u6f02\u6d6e\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u65b9\u6cd5\u5728\u8bed\u4e49\u5bf9\u9f50\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u8bed\u4e49\u548c\u7269\u7406\u771f\u5b9e\u6027\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u591f\u4fee\u6b63\u7269\u7406\u4e0d\u5408\u7406\u52a8\u4f5c\uff08\u5982\u811a\u90e8\u6f02\u6d6e\uff09\u800c\u4e0d\u7834\u574f\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faDistortion-aware Motion Calibrator (DMC)\uff0c\u8fd9\u662f\u4e00\u4e2a\u540e\u5904\u7406\u6a21\u5757\uff0c\u91c7\u7528\u81ea\u76d1\u7763\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u3002DMC\u5b66\u4e60\u5728\u7ed9\u5b9a\u6545\u610f\u626d\u66f2\u7684\u52a8\u4f5c\u548c\u539f\u59cb\u6587\u672c\u63cf\u8ff0\u65f6\uff0c\u751f\u6210\u7269\u7406\u5408\u7406\u7684\u52a8\u4f5c\uff0c\u800c\u4e0d\u4f9d\u8d56\u590d\u6742\u7269\u7406\u5efa\u6a21\u3002", "result": "DMC\u5728\u591a\u4e2a\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u6a21\u578b\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff1a\u5728T2M\u4e0aFID\u5206\u6570\u964d\u4f4e42.74%\uff0c\u5728T2M-GPT\u4e0a\u964d\u4f4e13.20%\uff0c\u540c\u65f6\u83b7\u5f97\u6700\u9ad8R-Precision\u3002\u5e94\u7528\u4e8eMoMask\u7b49\u9ad8\u8d28\u91cf\u6a21\u578b\u65f6\uff0c\u7a7f\u900f\u7387\u964d\u4f4e33.0%\uff0c\u6f02\u6d6e\u4f2a\u5f71\u66f4\u63a5\u8fd1\u771f\u5b9e\u53c2\u8003\u3002", "conclusion": "DMC\u4f5c\u4e3a\u4e00\u4e2a\u6709\u524d\u666f\u7684\u540e\u5904\u7406\u52a8\u4f5c\u7cbe\u70bc\u6846\u67b6\uff0c\u80fd\u591f\u4e3a\u5404\u7c7b\u6587\u672c\u5230\u52a8\u4f5c\u6a21\u578b\u540c\u65f6\u63d0\u5347\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u7269\u7406\u5408\u7406\u6027\u3002"}}
{"id": "2602.18252", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18252", "abs": "https://arxiv.org/abs/2602.18252", "authors": ["Rishika Bhagwatkar", "Irina Rish", "Nicolas Flammarion", "Francesco Croce"], "title": "On the Adversarial Robustness of Discrete Image Tokenizers", "comment": null, "summary": "Discrete image tokenizers encode visual inputs as sequences of tokens from a finite vocabulary and are gaining popularity in multimodal systems, including encoder-only, encoder-decoder, and decoder-only models. However, unlike CLIP encoders, their vulnerability to adversarial attacks has not been explored. Ours being the first work studying this topic, we first formulate attacks that aim to perturb the features extracted by discrete tokenizers, and thus change the extracted tokens. These attacks are computationally efficient, application-agnostic, and effective across classification, multimodal retrieval, and captioning tasks. Second, to defend against this vulnerability, inspired by recent work on robust CLIP encoders, we fine-tune popular tokenizers with unsupervised adversarial training, keeping all other components frozen. While unsupervised and task-agnostic, our approach significantly improves robustness to both unsupervised and end-to-end supervised attacks and generalizes well to unseen tasks and data. Unlike supervised adversarial training, our approach can leverage unlabeled images, making it more versatile. Overall, our work highlights the critical role of tokenizer robustness in downstream tasks and presents an important step in the development of safe multimodal foundation models.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7814\u7a76\u4e86\u79bb\u6563\u56fe\u50cf\u5206\u8bcd\u5668\u7684\u5bf9\u6297\u653b\u51fb\u8106\u5f31\u6027\uff0c\u63d0\u51fa\u4e86\u9ad8\u6548\u3001\u5e94\u7528\u65e0\u5173\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u65e0\u76d1\u7763\u5bf9\u6297\u8bad\u7ec3\u6765\u63d0\u5347\u5206\u8bcd\u5668\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u79bb\u6563\u56fe\u50cf\u5206\u8bcd\u5668\u5728\u591a\u6a21\u6001\u7cfb\u7edf\u4e2d\u8d8a\u6765\u8d8a\u6d41\u884c\uff0c\u4f46\u4e0eCLIP\u7f16\u7801\u5668\u4e0d\u540c\uff0c\u5176\u5bf9\u6297\u653b\u51fb\u8106\u5f31\u6027\u5c1a\u672a\u88ab\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\uff0c\u7814\u7a76\u79bb\u6563\u5206\u8bcd\u5668\u7684\u5b89\u5168\u6f0f\u6d1e\u5e76\u63d0\u51fa\u9632\u5fa1\u65b9\u6cd5\u3002", "method": "1. \u9996\u5148\u5236\u5b9a\u9488\u5bf9\u79bb\u6563\u5206\u8bcd\u5668\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u65e8\u5728\u6270\u52a8\u63d0\u53d6\u7684\u7279\u5f81\u5e76\u6539\u53d8\u63d0\u53d6\u7684token\uff1b2. \u63d0\u51fa\u9632\u5fa1\u65b9\u6cd5\uff1a\u53d7\u9c81\u68d2CLIP\u7f16\u7801\u5668\u7814\u7a76\u542f\u53d1\uff0c\u91c7\u7528\u65e0\u76d1\u7763\u5bf9\u6297\u8bad\u7ec3\u5fae\u8c03\u6d41\u884c\u5206\u8bcd\u5668\uff0c\u4fdd\u6301\u5176\u4ed6\u7ec4\u4ef6\u4e0d\u53d8\u3002", "result": "\u653b\u51fb\u65b9\u6cd5\u8ba1\u7b97\u9ad8\u6548\u3001\u5e94\u7528\u65e0\u5173\uff0c\u5728\u5206\u7c7b\u3001\u591a\u6a21\u6001\u68c0\u7d22\u548c\u5b57\u5e55\u751f\u6210\u4efb\u52a1\u4e2d\u5747\u6709\u6548\u3002\u9632\u5fa1\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u65e0\u76d1\u7763\u548c\u7aef\u5230\u7aef\u76d1\u7763\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u5230\u672a\u89c1\u4efb\u52a1\u548c\u6570\u636e\u3002", "conclusion": "\u672c\u6587\u5f3a\u8c03\u4e86\u5206\u8bcd\u5668\u9c81\u68d2\u6027\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u5f00\u53d1\u5b89\u5168\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002\u65e0\u76d1\u7763\u5bf9\u6297\u8bad\u7ec3\u76f8\u6bd4\u76d1\u7763\u65b9\u6cd5\u66f4\u5177\u901a\u7528\u6027\uff0c\u53ef\u5229\u7528\u672a\u6807\u8bb0\u56fe\u50cf\u3002"}}
{"id": "2602.18282", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18282", "abs": "https://arxiv.org/abs/2602.18282", "authors": ["Shiyan Du", "Conghan Yue", "Xinyu Cheng", "Dongyu Zhang"], "title": "DEIG: Detail-Enhanced Instance Generation with Fine-Grained Semantic Control", "comment": "Accepted by AAAI 2026", "summary": "Multi-Instance Generation has advanced significantly in spatial placement and attribute binding. However, existing approaches still face challenges in fine-grained semantic understanding, particularly when dealing with complex textual descriptions. To overcome these limitations, we propose DEIG, a novel framework for fine-grained and controllable multi-instance generation. DEIG integrates an Instance Detail Extractor (IDE) that transforms text encoder embeddings into compact, instance-aware representations, and a Detail Fusion Module (DFM) that applies instance-based masked attention to prevent attribute leakage across instances. These components enable DEIG to generate visually coherent multi-instance scenes that precisely match rich, localized textual descriptions. To support fine-grained supervision, we construct a high-quality dataset with detailed, compositional instance captions generated by VLMs. We also introduce DEIG-Bench, a new benchmark with region-level annotations and multi-attribute prompts for both humans and objects. Experiments demonstrate that DEIG consistently outperforms existing approaches across multiple benchmarks in spatial consistency, semantic accuracy, and compositional generalization. Moreover, DEIG functions as a plug-and-play module, making it easily integrable into standard diffusion-based pipelines.", "AI": {"tldr": "DEIG\u662f\u4e00\u4e2a\u7528\u4e8e\u7ec6\u7c92\u5ea6\u53ef\u63a7\u591a\u5b9e\u4f8b\u751f\u6210\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u4f8b\u7ec6\u8282\u63d0\u53d6\u5668\u548c\u7ec6\u8282\u878d\u5408\u6a21\u5757\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u6587\u672c\u63cf\u8ff0\u4e0b\u7684\u8bed\u4e49\u7406\u89e3\u95ee\u9898\uff0c\u5728\u7a7a\u95f4\u4e00\u81f4\u6027\u3001\u8bed\u4e49\u51c6\u786e\u6027\u548c\u7ec4\u5408\u6cdb\u5316\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u591a\u5b9e\u4f8b\u751f\u6210\u65b9\u6cd5\u5728\u7a7a\u95f4\u5e03\u5c40\u548c\u5c5e\u6027\u7ed1\u5b9a\u65b9\u9762\u5df2\u6709\u8fdb\u5c55\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u6587\u672c\u63cf\u8ff0\u65f6\u4ecd\u9762\u4e34\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7406\u89e3\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u9632\u6b62\u5b9e\u4f8b\u95f4\u5c5e\u6027\u6cc4\u6f0f\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faDEIG\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u5b9e\u4f8b\u7ec6\u8282\u63d0\u53d6\u5668(IDE)\uff0c\u5c06\u6587\u672c\u7f16\u7801\u5668\u5d4c\u5165\u8f6c\u6362\u4e3a\u7d27\u51d1\u7684\u5b9e\u4f8b\u611f\u77e5\u8868\u793a\uff1b2) \u7ec6\u8282\u878d\u5408\u6a21\u5757(DFM)\uff0c\u5e94\u7528\u57fa\u4e8e\u5b9e\u4f8b\u7684\u63a9\u7801\u6ce8\u610f\u529b\u673a\u5236\u9632\u6b62\u5b9e\u4f8b\u95f4\u5c5e\u6027\u6cc4\u6f0f\u3002\u8fd8\u6784\u5efa\u4e86\u5305\u542b\u8be6\u7ec6\u7ec4\u5408\u5b9e\u4f8b\u63cf\u8ff0\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86DEIG-Bench\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "DEIG\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u7a7a\u95f4\u4e00\u81f4\u6027\u3001\u8bed\u4e49\u51c6\u786e\u6027\u548c\u7ec4\u5408\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002\u8be5\u6846\u67b6\u53ef\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u96c6\u6210\u5230\u6807\u51c6\u57fa\u4e8e\u6269\u6563\u7684\u6d41\u7a0b\u4e2d\u3002", "conclusion": "DEIG\u901a\u8fc7\u521b\u65b0\u7684\u5b9e\u4f8b\u7ec6\u8282\u63d0\u53d6\u548c\u878d\u5408\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5bf9\u590d\u6742\u6587\u672c\u63cf\u8ff0\u7684\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7406\u89e3\uff0c\u80fd\u591f\u751f\u6210\u4e0e\u4e30\u5bcc\u5c40\u90e8\u5316\u6587\u672c\u63cf\u8ff0\u7cbe\u786e\u5339\u914d\u7684\u89c6\u89c9\u8fde\u8d2f\u591a\u5b9e\u4f8b\u573a\u666f\uff0c\u4e3a\u53ef\u63a7\u591a\u5b9e\u4f8b\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18309", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18309", "abs": "https://arxiv.org/abs/2602.18309", "authors": ["Ziyue Liu", "Davide Talon", "Federico Girella", "Zanxi Ruan", "Mattia Mondo", "Loris Bazzani", "Yiming Wang", "Marco Cristani"], "title": "Multi-Level Conditioning by Pairing Localized Text and Sketch for Fashion Image Generation", "comment": "Project page: https://intelligolabs.github.io/lots/", "summary": "Sketches offer designers a concise yet expressive medium for early-stage fashion ideation by specifying structure, silhouette, and spatial relationships, while textual descriptions complement sketches to convey material, color, and stylistic details. Effectively combining textual and visual modalities requires adherence to the sketch visual structure when leveraging the guidance of localized attributes from text. We present LOcalized Text and Sketch with multi-level guidance (LOTS), a framework that enhances fashion image generation by combining global sketch guidance with multiple localized sketch-text pairs. LOTS employs a Multi-level Conditioning Stage to independently encode local features within a shared latent space while maintaining global structural coordination. Then, the Diffusion Pair Guidance stage integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we develop Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Sketchy provides high-quality, clean sketches with a professional look and consistent structure. To assess robustness beyond this setting, we also include an \"in the wild\" split with non-expert sketches, featuring higher variability and imperfections. Experiments demonstrate that our method strengthens global structural adherence while leveraging richer localized semantic guidance, achieving improvement over state-of-the-art. The dataset, platform, and code are publicly available.", "AI": {"tldr": "LOTS\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u8349\u56fe\u5f15\u5bfc\u548c\u591a\u4e2a\u5c40\u90e8\u8349\u56fe-\u6587\u672c\u5bf9\u6765\u589e\u5f3a\u65f6\u5c1a\u56fe\u50cf\u751f\u6210\uff0c\u5728\u4fdd\u6301\u5168\u5c40\u7ed3\u6784\u7684\u540c\u65f6\u5229\u7528\u4e30\u5bcc\u7684\u5c40\u90e8\u8bed\u4e49\u6307\u5bfc\u3002", "motivation": "\u8349\u56fe\u4e3a\u8bbe\u8ba1\u5e08\u63d0\u4f9b\u4e86\u65e9\u671f\u65f6\u5c1a\u6784\u601d\u7684\u7b80\u6d01\u8868\u8fbe\u5a92\u4ecb\uff0c\u800c\u6587\u672c\u63cf\u8ff0\u5219\u8865\u5145\u4e86\u6750\u6599\u3001\u989c\u8272\u548c\u98ce\u683c\u7ec6\u8282\u3002\u6709\u6548\u7ed3\u5408\u6587\u672c\u548c\u89c6\u89c9\u6a21\u6001\u9700\u8981\u5728\u5229\u7528\u6587\u672c\u5c40\u90e8\u5c5e\u6027\u6307\u5bfc\u65f6\u9075\u5faa\u8349\u56fe\u89c6\u89c9\u7ed3\u6784\u3002", "method": "\u63d0\u51faLOTS\u6846\u67b6\uff0c\u5305\u542b\u591a\u7ea7\u6761\u4ef6\u9636\u6bb5\u72ec\u7acb\u7f16\u7801\u5c40\u90e8\u7279\u5f81\uff0c\u4ee5\u53ca\u6269\u6563\u5bf9\u5f15\u5bfc\u9636\u6bb5\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5728\u6269\u6563\u6a21\u578b\u53bb\u566a\u8fc7\u7a0b\u4e2d\u6574\u5408\u5c40\u90e8\u548c\u5168\u5c40\u6761\u4ef6\u3002\u540c\u65f6\u521b\u5efa\u4e86Sketchy\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e13\u4e1a\u8349\u56fe\u548c\u975e\u4e13\u5bb6\u8349\u56fe\u4e24\u79cd\u5206\u5272\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u589e\u5f3a\u4e86\u5168\u5c40\u7ed3\u6784\u9075\u5faa\u6027\uff0c\u540c\u65f6\u5229\u7528\u4e86\u66f4\u4e30\u5bcc\u7684\u5c40\u90e8\u8bed\u4e49\u6307\u5bfc\uff0c\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u53d6\u5f97\u4e86\u6539\u8fdb\u3002\u6570\u636e\u96c6\u3001\u5e73\u53f0\u548c\u4ee3\u7801\u5df2\u516c\u5f00\u3002", "conclusion": "LOTS\u6846\u67b6\u901a\u8fc7\u591a\u7ea7\u8349\u56fe-\u6587\u672c\u5f15\u5bfc\u6709\u6548\u63d0\u5347\u4e86\u65f6\u5c1a\u56fe\u50cf\u751f\u6210\u7684\u8d28\u91cf\uff0c\u5728\u4fdd\u6301\u7ed3\u6784\u4e00\u81f4\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u7cbe\u7ec6\u7684\u8bed\u4e49\u63a7\u5236\u3002"}}
{"id": "2602.18314", "categories": ["cs.CV", "cs.GR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18314", "abs": "https://arxiv.org/abs/2602.18314", "authors": ["Tianyi Song", "Danail Stoyanov", "Evangelos Mazomenos", "Francisco Vasconcelos"], "title": "Diff2DGS: Reliable Reconstruction of Occluded Surgical Scenes via 2D Gaussian Splatting", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Real-time reconstruction of deformable surgical scenes is vital for advancing robotic surgery, improving surgeon guidance, and enabling automation. Recent methods achieve dense reconstructions from da Vinci robotic surgery videos, with Gaussian Splatting (GS) offering real-time performance via graphics acceleration. However, reconstruction quality in occluded regions remains limited, and depth accuracy has not been fully assessed, as benchmarks like EndoNeRF and StereoMIS lack 3D ground truth. We propose Diff2DGS, a novel two-stage framework for reliable 3D reconstruction of occluded surgical scenes. In the first stage, a diffusion-based video module with temporal priors inpaints tissue occluded by instruments with high spatial-temporal consistency. In the second stage, we adapt 2D Gaussian Splatting (2DGS) with a Learnable Deformation Model (LDM) to capture dynamic tissue deformation and anatomical geometry. We also extend evaluation beyond prior image-quality metrics by performing quantitative depth accuracy analysis on the SCARED dataset. Diff2DGS outperforms state-of-the-art approaches in both appearance and geometry, reaching 38.02 dB PSNR on EndoNeRF and 34.40 dB on StereoMIS. Furthermore, our experiments demonstrate that optimizing for image quality alone does not necessarily translate into optimal 3D reconstruction accuracy. To address this, we further optimize the depth quality of the reconstructed 3D results, ensuring more faithful geometry in addition to high-fidelity appearance.", "AI": {"tldr": "Diff2DGS\uff1a\u7528\u4e8e\u624b\u672f\u573a\u666f\u5b9e\u65f63D\u91cd\u5efa\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u4fee\u590d\u88ab\u5668\u68b0\u906e\u6321\u7684\u7ec4\u7ec7\u533a\u57df\uff0c\u7ed3\u54082D\u9ad8\u65af\u6cfc\u6e85\u548c\u53ef\u5b66\u4e60\u53d8\u5f62\u6a21\u578b\u6355\u6349\u52a8\u6001\u7ec4\u7ec7\u53d8\u5f62\uff0c\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u51e0\u4f55\u7cbe\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5b9e\u65f6\u91cd\u5efa\u53ef\u53d8\u5f62\u624b\u672f\u573a\u666f\u5bf9\u63a8\u8fdb\u673a\u5668\u4eba\u624b\u672f\u3001\u6539\u5584\u5916\u79d1\u533b\u751f\u5f15\u5bfc\u548c\u5b9e\u73b0\u81ea\u52a8\u5316\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u906e\u6321\u533a\u57df\u7684\u91cd\u5efa\u8d28\u91cf\u6709\u9650\uff0c\u4e14\u7f3a\u4e4f\u6df1\u5ea6\u7cbe\u5ea6\u8bc4\u4f30\uff0c\u56e0\u4e3aEndoNeRF\u548cStereoMIS\u7b49\u57fa\u51c6\u7f3a\u4e4f3D\u771f\u5b9e\u6570\u636e\u3002", "method": "\u63d0\u51faDiff2DGS\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u9891\u6a21\u5757\uff0c\u5229\u7528\u65f6\u95f4\u5148\u9a8c\u4fee\u590d\u88ab\u5668\u68b0\u906e\u6321\u7684\u7ec4\u7ec7\uff0c\u4fdd\u6301\u65f6\u7a7a\u4e00\u81f4\u6027\uff1b\u7b2c\u4e8c\u9636\u6bb5\u91c7\u75282D\u9ad8\u65af\u6cfc\u6e85\uff082DGS\uff09\u7ed3\u5408\u53ef\u5b66\u4e60\u53d8\u5f62\u6a21\u578b\uff08LDM\uff09\u6355\u6349\u52a8\u6001\u7ec4\u7ec7\u53d8\u5f62\u548c\u89e3\u5256\u51e0\u4f55\u7ed3\u6784\u3002\u5728SCARED\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6df1\u5ea6\u7cbe\u5ea6\u5b9a\u91cf\u5206\u6790\u3002", "result": "\u5728EndoNeRF\u4e0a\u8fbe\u523038.02 dB PSNR\uff0c\u5728StereoMIS\u4e0a\u8fbe\u523034.40 dB PSNR\uff0c\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u51e0\u4f55\u7cbe\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5b9e\u9a8c\u8868\u660e\u4ec5\u4f18\u5316\u56fe\u50cf\u8d28\u91cf\u4e0d\u4e00\u5b9a\u80fd\u83b7\u5f97\u6700\u4f183D\u91cd\u5efa\u7cbe\u5ea6\uff0c\u56e0\u6b64\u8fdb\u4e00\u6b65\u4f18\u5316\u6df1\u5ea6\u8d28\u91cf\u4ee5\u786e\u4fdd\u51e0\u4f55\u4fdd\u771f\u5ea6\u3002", "conclusion": "Diff2DGS\u901a\u8fc7\u7ed3\u5408\u6269\u6563\u6a21\u578b\u4fee\u590d\u906e\u6321\u533a\u57df\u548c2D\u9ad8\u65af\u6cfc\u6e85\u6355\u6349\u52a8\u6001\u53d8\u5f62\uff0c\u5b9e\u73b0\u4e86\u624b\u672f\u573a\u666f\u7684\u9ad8\u8d28\u91cf3D\u91cd\u5efa\uff0c\u5728\u56fe\u50cf\u5916\u89c2\u548c\u51e0\u4f55\u7cbe\u5ea6\u65b9\u9762\u5747\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u673a\u5668\u4eba\u624b\u672f\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u76843D\u91cd\u5efa\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18322", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18322", "abs": "https://arxiv.org/abs/2602.18322", "authors": ["Ziteng Cui", "Shuhong Liu", "Xiaoyu Dong", "Xuangeng Chu", "Lin Gu", "Ming-Hsuan Yang", "Tatsuya Harada"], "title": "Unifying Color and Lightness Correction with View-Adaptive Curve Adjustment for Robust 3D Novel View Synthesis", "comment": "Journal extension version of CVPR 2025 paper: arXiv:2504.01503", "summary": "High-quality image acquisition in real-world environments remains challenging due to complex illumination variations and inherent limitations of camera imaging pipelines. These issues are exacerbated in multi-view capture, where differences in lighting, sensor responses, and image signal processor (ISP) configurations introduce photometric and chromatic inconsistencies that violate the assumptions of photometric consistency underlying modern 3D novel view synthesis (NVS) methods, including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), leading to degraded reconstruction and rendering quality. We propose Luminance-GS++, a 3DGS-based framework for robust NVS under diverse illumination conditions. Our method combines a globally view-adaptive lightness adjustment with a local pixel-wise residual refinement for precise color correction. We further design unsupervised objectives that jointly enforce lightness correction and multi-view geometric and photometric consistency. Extensive experiments demonstrate state-of-the-art performance across challenging scenarios, including low-light, overexposure, and complex luminance and chromatic variations. Unlike prior approaches that modify the underlying representation, our method preserves the explicit 3DGS formulation, improving reconstruction fidelity while maintaining real-time rendering efficiency.", "AI": {"tldr": "Luminance-GS++\uff1a\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u591a\u6837\u5316\u5149\u7167\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9c81\u68d2\u7684\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u901a\u8fc7\u5168\u5c40\u81ea\u9002\u5e94\u4eae\u5ea6\u8c03\u6574\u548c\u5c40\u90e8\u50cf\u7d20\u7ea7\u6b8b\u5dee\u7ec6\u5316\u89e3\u51b3\u591a\u89c6\u89d2\u91c7\u96c6\u4e2d\u7684\u5149\u5ea6\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u73af\u5883\u4e2d\u9ad8\u8d28\u91cf\u56fe\u50cf\u91c7\u96c6\u9762\u4e34\u590d\u6742\u5149\u7167\u53d8\u5316\u548c\u76f8\u673a\u6210\u50cf\u7ba1\u9053\u7684\u56fa\u6709\u5c40\u9650\uff0c\u591a\u89c6\u89d2\u91c7\u96c6\u4e2d\u7684\u5149\u7167\u3001\u4f20\u611f\u5668\u54cd\u5e94\u548cISP\u914d\u7f6e\u5dee\u5f02\u5bfc\u81f4\u5149\u5ea6\u548c\u8272\u5f69\u4e0d\u4e00\u81f4\uff0c\u8fd9\u8fdd\u53cd\u4e86\u73b0\u4ee33D\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u6cd5\uff08\u5982NeRF\u548c3DGS\uff09\u6240\u4f9d\u8d56\u7684\u5149\u5ea6\u4e00\u81f4\u6027\u5047\u8bbe\uff0c\u5bfc\u81f4\u91cd\u5efa\u548c\u6e32\u67d3\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u63d0\u51faLuminance-GS++\u6846\u67b6\uff0c\u7ed3\u5408\u5168\u5c40\u89c6\u89d2\u81ea\u9002\u5e94\u4eae\u5ea6\u8c03\u6574\u548c\u5c40\u90e8\u50cf\u7d20\u7ea7\u6b8b\u5dee\u7ec6\u5316\u8fdb\u884c\u7cbe\u786e\u8272\u5f69\u6821\u6b63\uff0c\u8bbe\u8ba1\u65e0\u76d1\u7763\u76ee\u6807\u51fd\u6570\u8054\u5408\u5f3a\u5236\u4eae\u5ea6\u6821\u6b63\u4e0e\u591a\u89c6\u89d2\u51e0\u4f55\u548c\u5149\u5ea6\u4e00\u81f4\u6027\uff0c\u4fdd\u6301\u663e\u5f0f3DGS\u8868\u793a\u800c\u4e0d\u4fee\u6539\u5e95\u5c42\u8868\u793a\u3002", "result": "\u5728\u4f4e\u5149\u7167\u3001\u8fc7\u66dd\u5149\u4ee5\u53ca\u590d\u6742\u4eae\u5ea6\u548c\u8272\u5f69\u53d8\u5316\u7b49\u6311\u6218\u6027\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u63d0\u9ad8\u4e86\u91cd\u5efa\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b9e\u65f6\u6e32\u67d3\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u89c6\u89d2\u91c7\u96c6\u4e2d\u7684\u5149\u5ea6\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5728\u4fdd\u63013DGS\u663e\u5f0f\u8868\u793a\u548c\u5b9e\u65f6\u6e32\u67d3\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u590d\u6742\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u65b0\u89c6\u89d2\u5408\u6210\u8d28\u91cf\u3002"}}
{"id": "2602.18329", "categories": ["cs.CV", "math.AT"], "pdf": "https://arxiv.org/pdf/2602.18329", "abs": "https://arxiv.org/abs/2602.18329", "authors": ["Qingsong Wang", "Jiaxing He", "Bingzhe Hou", "Tieru Wu", "Yang Cao", "Cailing Yao"], "title": "G-LoG Bi-filtration for Medical Image Classification", "comment": null, "summary": "Building practical filtrations on objects to detect topological and geometric features is an important task in the field of Topological Data Analysis (TDA). In this paper, leveraging the ability of the Laplacian of Gaussian operator to enhance the boundaries of medical images, we define the G-LoG (Gaussian-Laplacian of Gaussian) bi-filtration to generate the features more suitable for multi-parameter persistence module. By modeling volumetric images as bounded functions, then we prove the interleaving distance on the persistence modules obtained from our bi-filtrations on the bounded functions is stable with respect to the maximum norm of the bounded functions. Finally, we conduct experiments on the MedMNIST dataset, comparing our bi-filtration against single-parameter filtration and the established deep learning baselines, including Google AutoML Vision, ResNet, AutoKeras and auto-sklearn. Experiments results demonstrate that our bi-filtration significantly outperforms single-parameter filtration. Notably, a simple Multi-Layer Perceptron (MLP) trained on the topological features generated by our bi-filtration achieves performance comparable to complex deep learning models trained on the original dataset.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9ad8\u65af-\u62c9\u666e\u62c9\u65af\u7b97\u5b50(G-LoG)\u7684\u53cc\u53c2\u6570\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u62d3\u6251\u7279\u5f81\u63d0\u53d6\uff0c\u5728MedMNIST\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u5355\u53c2\u6570\u8fc7\u6ee4\uff0c\u4e14\u57fa\u4e8e\u62d3\u6251\u7279\u5f81\u7684\u7b80\u5355MLP\u6a21\u578b\u6027\u80fd\u63a5\u8fd1\u590d\u6742\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "motivation": "\u5728\u62d3\u6251\u6570\u636e\u5206\u6790\u4e2d\uff0c\u6784\u5efa\u5b9e\u7528\u7684\u8fc7\u6ee4\u65b9\u6cd5\u6765\u68c0\u6d4b\u62d3\u6251\u548c\u51e0\u4f55\u7279\u5f81\u662f\u4e00\u4e2a\u91cd\u8981\u4efb\u52a1\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u62c9\u666e\u62c9\u65af\u9ad8\u65af\u7b97\u5b50\u589e\u5f3a\u533b\u5b66\u56fe\u50cf\u8fb9\u754c\u7684\u80fd\u529b\uff0c\u5b9a\u4e49\u66f4\u9002\u5408\u591a\u53c2\u6570\u6301\u4e45\u6027\u6a21\u5757\u7684\u7279\u5f81\u3002", "method": "\u63d0\u51faG-LoG\uff08\u9ad8\u65af-\u62c9\u666e\u62c9\u65af\u9ad8\u65af\uff09\u53cc\u53c2\u6570\u8fc7\u6ee4\u65b9\u6cd5\uff1a1\uff09\u5229\u7528\u62c9\u666e\u62c9\u65af\u9ad8\u65af\u7b97\u5b50\u589e\u5f3a\u533b\u5b66\u56fe\u50cf\u8fb9\u754c\uff1b2\uff09\u5c06\u4f53\u79ef\u56fe\u50cf\u5efa\u6a21\u4e3a\u6709\u754c\u51fd\u6570\uff1b3\uff09\u8bc1\u660e\u4ece\u53cc\u8fc7\u6ee4\u83b7\u5f97\u7684\u6301\u4e45\u6027\u6a21\u5757\u7684\u4ea4\u9519\u8ddd\u79bb\u76f8\u5bf9\u4e8e\u6709\u754c\u51fd\u6570\u7684\u6700\u5927\u8303\u6570\u662f\u7a33\u5b9a\u7684\u3002", "result": "\u5728MedMNIST\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff1a1\uff09G-LoG\u53cc\u53c2\u6570\u8fc7\u6ee4\u663e\u8457\u4f18\u4e8e\u5355\u53c2\u6570\u8fc7\u6ee4\uff1b2\uff09\u57fa\u4e8e\u53cc\u8fc7\u6ee4\u751f\u6210\u62d3\u6251\u7279\u5f81\u7684\u7b80\u5355\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\u6027\u80fd\u4e0e\u5728\u539f\u59cb\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u590d\u6742\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08Google AutoML Vision\u3001ResNet\u3001AutoKeras\u3001auto-sklearn\uff09\u76f8\u5f53\u3002", "conclusion": "G-LoG\u53cc\u53c2\u6570\u8fc7\u6ee4\u65b9\u6cd5\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u62d3\u6251\u7279\u5f81\u63d0\u53d6\u5de5\u5177\uff0c\u80fd\u591f\u751f\u6210\u9002\u5408\u591a\u53c2\u6570\u6301\u4e45\u6027\u6a21\u5757\u7684\u7279\u5f81\uff0c\u4e14\u5177\u6709\u7406\u8bba\u7a33\u5b9a\u6027\u4fdd\u8bc1\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u7b80\u5316\u4e86\u6a21\u578b\u590d\u6742\u5ea6\u3002"}}
{"id": "2602.18422", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18422", "abs": "https://arxiv.org/abs/2602.18422", "authors": ["Linxi Xie", "Lisong C. Sun", "Ashley Neall", "Tong Wu", "Shengqu Cai", "Gordon Wetzstein"], "title": "Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control", "comment": "Project page here: https://codeysun.github.io/generated-reality", "summary": "Extended reality (XR) demands generative models that respond to users' tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video world model that is conditioned on both tracked head pose and joint-level hand poses. For this purpose, we evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control, enabling dexterous hand--object interactions. We train a bidirectional video diffusion model teacher using this strategy and distill it into a causal, interactive system that generates egocentric virtual environments. We evaluate this generated reality system with human subjects and demonstrate improved task performance as well as a significantly higher level of perceived amount of control over the performed actions compared with relevant baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5934\u624b\u59ff\u6001\u63a7\u5236\u7684\u4eba\u4e3a\u4e2d\u5fc3\u89c6\u9891\u4e16\u754c\u6a21\u578b\uff0c\u7528\u4e8e\u6269\u5c55\u73b0\u5b9e(XR)\u4e2d\u7684\u4ea4\u4e92\u5f0f\u73af\u5883\u751f\u6210", "motivation": "\u5f53\u524d\u89c6\u9891\u4e16\u754c\u6a21\u578b\u53ea\u80fd\u63a5\u53d7\u6587\u672c\u6216\u952e\u76d8\u7b49\u7c97\u7c92\u5ea6\u63a7\u5236\u4fe1\u53f7\uff0c\u65e0\u6cd5\u54cd\u5e94XR\u4e2d\u7528\u6237\u5b9e\u65f6\u8ffd\u8e2a\u7684\u771f\u5b9e\u4e16\u754c\u8fd0\u52a8\uff0c\u9650\u5236\u4e86\u5176\u5728\u5177\u8eab\u4ea4\u4e92\u4e2d\u7684\u5b9e\u7528\u6027", "method": "1) \u8bc4\u4f30\u73b0\u6709\u6269\u6563\u53d8\u6362\u5668\u6761\u4ef6\u7b56\u7565\uff1b2) \u63d0\u51fa\u6709\u6548\u76843D\u5934\u90e8\u548c\u624b\u90e8\u59ff\u6001\u63a7\u5236\u673a\u5236\uff1b3) \u8bad\u7ec3\u53cc\u5411\u89c6\u9891\u6269\u6563\u6a21\u578b\u6559\u5e08\uff0c\u5e76\u84b8\u998f\u4e3a\u56e0\u679c\u4ea4\u4e92\u7cfb\u7edf\u751f\u6210\u7b2c\u4e00\u4eba\u79f0\u865a\u62df\u73af\u5883", "result": "\u901a\u8fc7\u4eba\u7c7b\u53d7\u8bd5\u8005\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4\u76f8\u5173\u57fa\u7ebf\uff0c\u8be5\u7cfb\u7edf\u5728\u4efb\u52a1\u6027\u80fd\u4e0a\u6709\u6539\u8fdb\uff0c\u4e14\u7528\u6237\u611f\u77e5\u5230\u7684\u52a8\u4f5c\u63a7\u5236\u6c34\u5e73\u663e\u8457\u66f4\u9ad8", "conclusion": "\u63d0\u51fa\u7684\u5934\u624b\u59ff\u6001\u6761\u4ef6\u5316\u89c6\u9891\u4e16\u754c\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u7075\u5de7\u7684\u624b-\u7269\u4ea4\u4e92\uff0c\u4e3aXR\u4e2d\u7684\u5177\u8eab\u4ea4\u4e92\u63d0\u4f9b\u4e86\u66f4\u81ea\u7136\u548c\u7cbe\u786e\u7684\u63a7\u5236\u65b9\u5f0f"}}
{"id": "2602.18424", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18424", "abs": "https://arxiv.org/abs/2602.18424", "authors": ["Xia Su", "Ruiqi Chen", "Benlin Liu", "Jingwei Ma", "Zonglin Di", "Ranjay Krishna", "Jon Froehlich"], "title": "CapNav: Benchmarking Vision Language Models on Capability-conditioned Indoor Navigation", "comment": null, "summary": "Vision-Language Models (VLMs) have shown remarkable progress in Vision-Language Navigation (VLN), offering new possibilities for navigation decision-making that could benefit both robotic platforms and human users. However, real-world navigation is inherently conditioned by the agent's mobility constraints. For example, a sweeping robot cannot traverse stairs, while a quadruped can. We introduce Capability-Conditioned Navigation (CapNav), a benchmark designed to evaluate how well VLMs can navigate complex indoor spaces given an agent's specific physical and operational capabilities. CapNav defines five representative human and robot agents, each described with physical dimensions, mobility capabilities, and environmental interaction abilities. CapNav provides 45 real-world indoor scenes, 473 navigation tasks, and 2365 QA pairs to test if VLMs can traverse indoor environments based on agent capabilities. We evaluate 13 modern VLMs and find that current VLM's navigation performance drops sharply as mobility constraints tighten, and that even state-of-the-art models struggle with obstacle types that require reasoning on spatial dimensions. We conclude by discussing the implications for capability-aware navigation and the opportunities for advancing embodied spatial reasoning in future VLMs. The benchmark is available at https://github.com/makeabilitylab/CapNav", "AI": {"tldr": "CapNav\u662f\u4e00\u4e2a\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8003\u8651\u667a\u80fd\u4f53\u7269\u7406\u80fd\u529b\u7ea6\u675f\u4e0b\u5bfc\u822a\u6027\u80fd\u7684\u65b0\u57fa\u51c6\uff0c\u5b9a\u4e49\u4e865\u79cd\u4ee3\u8868\u6027\u667a\u80fd\u4f53\uff0c\u5305\u542b45\u4e2a\u771f\u5b9e\u5ba4\u5185\u573a\u666f\u548c473\u4e2a\u5bfc\u822a\u4efb\u52a1\uff0c\u53d1\u73b0\u5f53\u524dVLM\u5728\u79fb\u52a8\u7ea6\u675f\u589e\u52a0\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d", "motivation": "\u73b0\u5b9e\u4e16\u754c\u5bfc\u822a\u51b3\u7b56\u9700\u8981\u8003\u8651\u667a\u80fd\u4f53\u7684\u7269\u7406\u80fd\u529b\u7ea6\u675f\uff08\u5982\u626b\u5730\u673a\u5668\u4eba\u4e0d\u80fd\u722c\u697c\u68af\uff0c\u56db\u8db3\u673a\u5668\u4eba\u53ef\u4ee5\uff09\uff0c\u4f46\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u7814\u7a76\u7f3a\u4e4f\u5bf9\u667a\u80fd\u4f53\u5177\u4f53\u80fd\u529b\u7684\u8003\u8651\uff0c\u9700\u8981\u5efa\u7acb\u80fd\u529b\u6761\u4ef6\u5bfc\u822a\u57fa\u51c6\u6765\u8bc4\u4f30VLM\u7684\u5b9e\u9645\u5e94\u7528\u80fd\u529b", "method": "\u5b9a\u4e49\u4e865\u79cd\u4ee3\u8868\u6027\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u667a\u80fd\u4f53\uff0c\u63cf\u8ff0\u5176\u7269\u7406\u5c3a\u5bf8\u3001\u79fb\u52a8\u80fd\u529b\u548c\u73af\u5883\u4ea4\u4e92\u80fd\u529b\uff1b\u6784\u5efa\u4e86\u5305\u542b45\u4e2a\u771f\u5b9e\u5ba4\u5185\u573a\u666f\u3001473\u4e2a\u5bfc\u822a\u4efb\u52a1\u548c2365\u4e2a\u95ee\u7b54\u5bf9\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff1b\u8bc4\u4f30\u4e8613\u4e2a\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u80fd\u529b\u7ea6\u675f\u6761\u4ef6\u4e0b\u7684\u5bfc\u822a\u6027\u80fd", "result": "\u5f53\u524dVLM\u7684\u5bfc\u822a\u6027\u80fd\u968f\u7740\u79fb\u52a8\u7ea6\u675f\u7684\u6536\u7d27\u800c\u6025\u5267\u4e0b\u964d\uff1b\u5373\u4f7f\u662f\u5148\u8fdb\u7684\u6a21\u578b\u5728\u5904\u7406\u9700\u8981\u7a7a\u95f4\u7ef4\u5ea6\u63a8\u7406\u7684\u969c\u788d\u7c7b\u578b\u65f6\u4e5f\u8868\u73b0\u4e0d\u4f73\uff1b\u6a21\u578b\u5728\u8003\u8651\u667a\u80fd\u4f53\u5177\u4f53\u80fd\u529b\u65f6\u7684\u5bfc\u822a\u51b3\u7b56\u80fd\u529b\u6709\u9650", "conclusion": "\u9700\u8981\u5f00\u53d1\u80fd\u529b\u611f\u77e5\u7684\u5bfc\u822a\u7cfb\u7edf\uff0c\u672a\u6765VLM\u9700\u8981\u52a0\u5f3a\u5177\u8eab\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff1bCapNav\u57fa\u51c6\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdbVLM\u5728\u5b9e\u9645\u7ea6\u675f\u6761\u4ef6\u4e0b\u7684\u5bfc\u822a\u6027\u80fd\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177"}}
{"id": "2602.18434", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18434", "abs": "https://arxiv.org/abs/2602.18434", "authors": ["Vatsal Agarwal", "Saksham Suri", "Matthew Gwilliam", "Pulkit Kumar", "Abhinav Shrivastava"], "title": "Going Down Memory Lane: Scaling Tokens for Video Stream Understanding with Dynamic KV-Cache Memory", "comment": "Project page: see https://vatsalag99.github.io/memstream/", "summary": "Streaming video understanding requires models to robustly encode, store, and retrieve information from a continuous video stream to support accurate video question answering (VQA). Existing state-of-the-art approaches rely on key-value caching to accumulate frame-level information over time, but use a limited number of tokens per frame, leading to the loss of fine-grained visual details. In this work, we propose scaling the token budget to enable more granular spatiotemporal understanding and reasoning. First, we find that current methods are ill-equipped to handle dense streams: their feature encoding causes query-frame similarity scores to increase over time, biasing retrieval toward later frames. To address this, we introduce an adaptive selection strategy that reduces token redundancy while preserving local spatiotemporal information. We further propose a training-free retrieval mixture-of-experts that leverages external models to better identify relevant frames. Our method, MemStream, achieves +8.0% on CG-Bench, +8.5% on LVBench, and +2.4% on VideoMME (Long) over ReKV with Qwen2.5-VL-7B.", "AI": {"tldr": "MemStream\u901a\u8fc7\u589e\u52a0token\u9884\u7b97\u3001\u81ea\u9002\u5e94\u9009\u62e9\u7b56\u7565\u548c\u65e0\u8bad\u7ec3\u68c0\u7d22\u4e13\u5bb6\u6df7\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d41\u5f0f\u89c6\u9891\u7406\u89e3\u6027\u80fd", "motivation": "\u73b0\u6709\u6d41\u5f0f\u89c6\u9891\u7406\u89e3\u65b9\u6cd5\u4f7f\u7528\u6709\u9650\u7684\u6bcf\u5e27token\u6570\uff0c\u5bfc\u81f4\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7ec6\u8282\u4e22\u5931\uff0c\u4e14\u65e0\u6cd5\u6709\u6548\u5904\u7406\u5bc6\u96c6\u89c6\u9891\u6d41\uff0c\u5b58\u5728\u67e5\u8be2-\u5e27\u76f8\u4f3c\u5ea6\u968f\u65f6\u95f4\u589e\u52a0\u800c\u504f\u5411\u540e\u671f\u5e27\u7684\u504f\u5dee", "method": "1. \u589e\u52a0token\u9884\u7b97\u4ee5\u5b9e\u73b0\u66f4\u7ec6\u7c92\u5ea6\u7684\u65f6\u7a7a\u7406\u89e3\uff1b2. \u5f15\u5165\u81ea\u9002\u5e94\u9009\u62e9\u7b56\u7565\u51cf\u5c11token\u5197\u4f59\u540c\u65f6\u4fdd\u7559\u5c40\u90e8\u65f6\u7a7a\u4fe1\u606f\uff1b3. \u63d0\u51fa\u65e0\u8bad\u7ec3\u68c0\u7d22\u4e13\u5bb6\u6df7\u5408\uff0c\u5229\u7528\u5916\u90e8\u6a21\u578b\u66f4\u597d\u5730\u8bc6\u522b\u76f8\u5173\u5e27", "result": "\u5728CG-Bench\u4e0a\u63d0\u53478.0%\uff0cLVBench\u4e0a\u63d0\u53478.5%\uff0cVideoMME(Long)\u4e0a\u63d0\u53472.4%\uff08\u57fa\u4e8eQwen2.5-VL-7B\u6a21\u578b\uff09", "conclusion": "MemStream\u901a\u8fc7\u6269\u5c55token\u9884\u7b97\u548c\u521b\u65b0\u7684\u81ea\u9002\u5e94\u9009\u62e9\u4e0e\u68c0\u7d22\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d41\u5f0f\u89c6\u9891\u95ee\u7b54\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5bc6\u96c6\u89c6\u9891\u6d41\u65f6\u7684\u5c40\u9650\u6027"}}
